{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e90c434e-c20d-41c4-929e-8cfe6da4ea13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install uv opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e5c08f8-d0c6-43ff-b231-05bab5d86253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_pat = dbutils.secrets.get(\"justinm-buildathon-secrets\", \"hf_pat\")\n",
    "os.environ[\"HF_TOKEN\"] = hf_pat\n",
    "login(token=hf_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b15e6f-b13c-41b8-97f4-a3415255090e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Sam3Processor, Sam3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e37f23fa-7e68-439b-b27d-ae32803af5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from transformers import Sam3Processor, Sam3Model\n",
    "\n",
    "class SAM3(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    MLflow wrapper for Hugging Face Transformers SAM3 models\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        import torch\n",
    "        import numpy as np\n",
    "        import requests\n",
    "        from PIL import Image\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "        from transformers import Sam3Processor, Sam3Model, logging\n",
    "        from huggingface_hub import login\n",
    "        import sys\n",
    "\n",
    "        if not hasattr(sys.stdout, \"isatty\"):\n",
    "            sys.stdout.isatty = lambda: False\n",
    "\n",
    "        logging.set_verbosity_error()\n",
    "        logging.disable_progress_bar()\n",
    "\n",
    "        hf_pat = os.environ[\"HF_TOKEN\"]\n",
    "        login(token=hf_pat)\n",
    "\n",
    "        \"\"\"Load Transformers SAM3 model\"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = Sam3Model.from_pretrained(\"facebook/sam3\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(self.device)\n",
    "        self.processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n",
    "\n",
    "    def segment(self, image: Image.Image, text: str, threshold: float, mask_threshold: float):\n",
    "        \"\"\"\n",
    "        Perform promptable concept segmentation using SAM3.\n",
    "        Returns format compatible with gr.AnnotatedImage: (image, [(mask, label), ...])\n",
    "        \"\"\"\n",
    "        if image is None:\n",
    "            return None, \"❌ Please upload an image.\"\n",
    "        \n",
    "        if not text.strip():\n",
    "            return (image, []), \"❌ Please enter a text prompt.\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.processor(images=image, text=text.strip(), return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            for key in inputs:\n",
    "                if inputs[key].dtype == torch.float32:\n",
    "                    inputs[key] = inputs[key].to(self.model.dtype)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            results = self.processor.post_process_instance_segmentation(\n",
    "                outputs,\n",
    "                threshold=threshold,\n",
    "                mask_threshold=mask_threshold,\n",
    "                target_sizes=inputs.get(\"original_sizes\").tolist()\n",
    "            )[0]\n",
    "            \n",
    "            n_masks = len(results['masks'])\n",
    "            if n_masks == 0:\n",
    "                return (image, []), f\"❌ No objects found matching '{text}' (try adjusting thresholds).\"\n",
    "            \n",
    "            # Format for AnnotatedImage: list of (mask, label) tuples\n",
    "            # mask should be numpy array with values 0-1 (float) matching image dimensions\n",
    "            annotations = []\n",
    "            for i, (mask, score) in enumerate(zip(results['masks'], results['scores'])):\n",
    "                # Convert binary mask to float numpy array (0-1 range)\n",
    "                mask_np = mask.cpu().numpy().astype(np.float32)\n",
    "                label = f\"{text} #{i+1} ({score:.2f})\"\n",
    "                annotations.append((mask_np, label))\n",
    "            \n",
    "            scores_text = \", \".join([f\"{s:.2f}\" for s in results['scores'].cpu().numpy()[:5]])\n",
    "            info = f\"✅ Found **{n_masks}** objects matching **'{text}'**\\nConfidence scores: {scores_text}{'...' if n_masks > 5 else ''}\"\n",
    "            \n",
    "            # Return tuple: (base_image, list_of_annotations)\n",
    "            return annotations, info\n",
    "            # return (image, annotations), info\n",
    "        \n",
    "        except Exception as e:\n",
    "            return ([]), f\"❌ Error during segmentation: {str(e)}\"\n",
    "            # return (image, []), f\"❌ Error during segmentation: {str(e)}\"\n",
    "          \n",
    "    def convert_image(self, image_path):\n",
    "        if image_path.startswith(\"http\"):\n",
    "            image = Image.open(requests.get(image_path, stream=True).raw).convert(\"RGB\")\n",
    "        elif image_path.startswith('/'):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        elif image_path.startswith('data:image'):\n",
    "            header, b64_string = image_path.split(\",\", 1)\n",
    "            image_bytes = base64.b64decode(b64_string)\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        elif isinstance(image_path, str):\n",
    "            image_bytes = base64.b64decode(image_path)\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        elif isinstance(image_path, bytes):\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        else:\n",
    "            return \"❌ Unsupported image type\"\n",
    "        return image\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            # print(\"pandas\")\n",
    "            image_path = model_input['image_path'].tolist()[0]\n",
    "            prompt = model_input[\"prompt\"].tolist()[0]\n",
    "        else:\n",
    "            # print(\"dict\")\n",
    "            image_path = model_input[\"image_path\"]\n",
    "            prompt = model_input[\"prompt\"]\n",
    "\n",
    "        image = self.convert_image(image_path)\n",
    "        if image == \"❌ Unsupported image type\":\n",
    "            return image\n",
    "        \n",
    "        return self.segment(image, prompt, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3abd4ba4-de25-4920-b914-3fecbc65af27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sam = SAM3()\n",
    "\n",
    "class ContextObject():\n",
    "  def __init__(self, artifacts):\n",
    "    self.artifacts = None\n",
    "\n",
    "artifacts = None\n",
    "sam_context = ContextObject(artifacts)\n",
    "\n",
    "sam.load_context(context = sam_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "522bdc33-7332-4ab6-b88d-72fe21ac53e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return base64_string\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/Volumes/pubsec_video_processing/cv/images/bruno.png\"\n",
    "image_path = image_to_base64(image_path)\n",
    "prompt = \"weimaraner\"\n",
    "model_input = {\n",
    "  \"image_path\": image_path,\n",
    "  \"prompt\": prompt\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = sam.predict(\n",
    "  context = None,\n",
    "  model_input = model_input,\n",
    "  params = None\n",
    ")\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# base64_string = image_to_base64(image_path)\n",
    "# # print(base64_string)\n",
    "\n",
    "# starting_time = timeit.default_timer()\n",
    "# model_input = {\n",
    "#   \"image\": base64_string\n",
    "# }\n",
    "# response = yolo.predict(\n",
    "#   context = None,\n",
    "#   model_input = model_input,\n",
    "#   params = None\n",
    "# )\n",
    "# print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d13ac77-1590-4121-be2c-53ffc0bd470e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_image(image_path):\n",
    "    if image_path.startswith(\"http\"):\n",
    "        image = Image.open(requests.get(image_path, stream=True).raw).convert(\"RGB\")\n",
    "    elif image_path.startswith('/'):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    elif image_path.startswith('data:image'):\n",
    "        header, b64_string = image_path.split(\",\", 1)\n",
    "        image_bytes = base64.b64decode(b64_string)\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "    elif isinstance(image_path, str):\n",
    "        image_bytes = base64.b64decode(image_path)\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "    elif isinstance(image_path, bytes):\n",
    "        image = Image.open(BytesIO(image_bytes))\n",
    "    else:\n",
    "        return \"❌ Unsupported image type\"\n",
    "    return image\n",
    "\n",
    "# # Unpack your data structure\n",
    "# img = response[0][0]\n",
    "# mask_list = response[0][1]\n",
    "# message = response[1]\n",
    "# # ((img, mask_list), message) = segmented_image[0]  # whatever your variable is called\n",
    "# mask, label = mask_list[0]       # first mask\n",
    "\n",
    "# Unpack your data structure\n",
    "image_path = model_input['image_path']\n",
    "# if image_path.startswith(\"http\"):\n",
    "#     img = Image.open(requests.get(image_path, stream=True).raw).convert(\"RGB\")\n",
    "# else:\n",
    "#     img = Image.open(image_path).convert(\"RGB\")\n",
    "img = convert_image(image_path)\n",
    "\n",
    "mask_list = response[0]\n",
    "message = response[1]\n",
    "# ((img, mask_list), message) = segmented_image[0]  # whatever your variable is called\n",
    "mask, label = mask_list[0]       # first mask\n",
    "\n",
    "# Get mask in 0–255 range\n",
    "mask_uint8 = (mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Convert mask to RGBA (red overlay)\n",
    "overlay = Image.fromarray(mask_uint8, mode=\"L\").convert(\"RGBA\")\n",
    "overlay_data = overlay.load()\n",
    "\n",
    "# Color the mask red with alpha proportional to mask value\n",
    "for y in range(overlay.height):\n",
    "    for x in range(overlay.width):\n",
    "        a = overlay_data[x, y][0]  # original grayscale value = alpha\n",
    "        overlay_data[x, y] = (255, 0, 0, int(a/2))  # red with transparency\n",
    "\n",
    "# Composite\n",
    "result = Image.alpha_composite(img.convert(\"RGBA\"), overlay)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79385c33-1ee2-46d1-9d8b-11c95d69d823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# specify the location the model will be saved/registered in Unity Catalog\n",
    "catalog = \"pubsec_video_processing\"\n",
    "schema = \"cv\"\n",
    "model_name = \"transformers-sam3\"\n",
    "model_full_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "signature = infer_signature(model_input=model_input, model_output=response)\n",
    "\n",
    "# Define conda environment with dependencies\n",
    "conda_env = {\n",
    "    'channels': ['conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.12.3',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'mlflow>=2.10.0',\n",
    "                'torch>=2.0.0',\n",
    "                'git+https://github.com/huggingface/transformers.git',\n",
    "                'Pillow',\n",
    "                'torchvision',\n",
    "                \"cloudpickle==3.0.0\",\n",
    "                # 'pillow>=9.0.0',\n",
    "                'numpy>=1.23.0',\n",
    "                'pandas>=1.5.0',\n",
    "                'accelerate>=0.20.0'\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    'name': 'sam3_tracker_env'\n",
    "}\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=SAM3(),\n",
    "        signature=signature,\n",
    "        conda_env=conda_env,\n",
    "        # extra_pip_requirements=[\n",
    "        #   \"torch\",\n",
    "        #   \"git+https://github.com/huggingface/transformers.git\",\n",
    "        #   \"Pillow\"\n",
    "        # ]\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Model registered! URI: runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46e9c989-4ab3-4fda-b7cf-f65db70990ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41d8963-e9d1-4a77-81ef-aca365b9ee15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1b5fde-d10f-4860-9595-dba7cc00dbce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input = {\n",
    "  'model_input': [model_input]\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame([model_input])\n",
    "print(input_df)\n",
    "\n",
    "input = model_input\n",
    "print(input)\n",
    "\n",
    "result = mlflow.models.predict(\n",
    "  model_uri = model_uri,\n",
    "  # input_data = input_df,\n",
    "  input_data = input_df,\n",
    "  content_type=\"json\",\n",
    "  install_mlflow=False,\n",
    "  env_manager='uv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9db88d-d287-4c6b-b7ea-a5efbbce2f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the model using the \"run\" from above.\n",
    "mlflow.register_model(model_uri=f\"runs:/{run_id}/model\", name=model_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "226c707a-6b9f-484e-9bfd-8693dcd27dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "    EndpointCoreConfigInput,\n",
    "    ServedModelInput,\n",
    "    ServedModelInputWorkloadSize,\n",
    "    ServedModelInputWorkloadType,\n",
    "    ServingEndpointDetailed,\n",
    ")\n",
    "\n",
    "# name the endpoint\n",
    "endpoint_name = model_full_name.replace(\".\",\"_\") + \"_endpoint\"\n",
    "using_gpu = True\n",
    "\n",
    "# specify Unity Catalog as the model registry\n",
    "mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "\n",
    "# Use the MlflowClient to get a list of all versions for the registered model in Unity Catalog\n",
    "all_versions = mlflow_client.search_model_versions(f\"name='{model_full_name}'\")\n",
    "# then sort the list of versions by version number and get the latest version\n",
    "latest_version = max([int(v.version) for v in all_versions])\n",
    "# finally, use the MlflowClient to get the latest version of the registered model in Unity Catalog\n",
    "latest_model = mlflow_client.get_model_version(model_full_name, str(latest_version))\n",
    "\n",
    "# use databricks-sdk to launch the model serving endpoint\n",
    "wc = WorkspaceClient()\n",
    "served_models =[ServedModelInput(model_name=model_full_name, scale_to_zero_enabled=True,\n",
    "                                environment_vars={\"HF_TOKEN\":\"{{secrets/justinm-buildathon-secrets/hf_pat}}\"},\n",
    "                                model_version=latest_model.version,\n",
    "                                workload_size=ServedModelInputWorkloadSize.SMALL, # compute workload size for necessary concurrency \n",
    "                                workload_type=ServedModelInputWorkloadType.GPU_SMALL if using_gpu else ServedModelInputWorkloadType.CPU, # CPU or GPU workload type\n",
    "                                )]\n",
    "                                # scale_to_zero_enabled=True)]\n",
    "\n",
    "try:\n",
    "    print(f'Creating endpoint {endpoint_name} with latest version...')\n",
    "    wc.serving_endpoints.create_and_wait(endpoint_name, config=EndpointCoreConfigInput(served_models=served_models), timeout=datetime.timedelta(seconds=3600))\n",
    "    print(f'Endpoint created: {endpoint_name}')\n",
    "except Exception as e:\n",
    "    if 'already exists' in str(e):\n",
    "        # print(f'Endpoint exists, skipping...')\n",
    "        print(f'Endpoint exists, updating with latest model version...')\n",
    "        wc.serving_endpoints.update_config_and_wait(endpoint_name, served_models=served_models, timeout=datetime.timedelta(seconds=3600))\n",
    "    else: \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "607696e9-6952-4291-a574-d46792a4f2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import timeit\n",
    "import pandas as pd\n",
    "\n",
    "# DATABRICKS_TOKEN = dbutils.secrets.get(scope=\"your-secrets\", key=\"your_pat\")\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "DATABRICKS_HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().toString()[5:-1]\n",
    "try:\n",
    "  endpoint_name = model_full_name.replace(\".\",\"_\") + \"_endpoint\"\n",
    "except:\n",
    "  endpoint_name = \"pubsec_video_processing_cv_transformers-sam3_endpoint\"\n",
    "\n",
    "image_path = \"/Volumes/pubsec_video_processing/cv/images/bruno.png\"\n",
    "image_path = \"https://www.prodograw.com/wp-content/uploads/2025/09/Weimaraner-2-1.jpg\"\n",
    "prompt = \"weimaraner\"\n",
    "model_input = {\n",
    "  \"image_path\": image_path,\n",
    "  \"prompt\": prompt\n",
    "}\n",
    "\n",
    "input = {\n",
    "  'inputs': [model_input]\n",
    "}\n",
    "\n",
    "headers = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\"}\n",
    "url = f'https://{DATABRICKS_HOST}/serving-endpoints/{endpoint_name}/invocations'\n",
    "\n",
    "print(url)\n",
    "starting_time = timeit.default_timer()\n",
    "response = requests.post(\n",
    "    url=url, json=input, headers=headers\n",
    ")\n",
    "print(f\"\\nInference time, end 2 end :{round((timeit.default_timer() - starting_time)*1000)}ms\\n\")\n",
    "\n",
    "print(json.dumps(response.json()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5994fb6a-f97d-432c-b8a7-08bddcb8456f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "res = response.json()['predictions']\n",
    "\n",
    "# Unpack your data structure\n",
    "image_path = model_input['image_path']\n",
    "if image_path.startswith(\"http\"):\n",
    "    img = Image.open(requests.get(image_path, stream=True).raw).convert(\"RGB\")\n",
    "else:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "mask_list = res[0]\n",
    "message = res[1]\n",
    "# ((img, mask_list), message) = segmented_image[0]  # whatever your variable is called\n",
    "mask, label = mask_list[0]       # first mask\n",
    "\n",
    "# Get mask in 0–255 range\n",
    "import numpy as np\n",
    "mask_uint8 = (np.array(mask) * 255).astype(\"uint8\")\n",
    "\n",
    "\n",
    "# Convert mask to RGBA (red overlay)\n",
    "overlay = Image.fromarray(mask_uint8, mode=\"L\").convert(\"RGBA\")\n",
    "overlay_data = overlay.load()\n",
    "\n",
    "# Color the mask red with alpha proportional to mask value\n",
    "for y in range(overlay.height):\n",
    "    for x in range(overlay.width):\n",
    "        a = overlay_data[x, y][0]  # original grayscale value = alpha\n",
    "        overlay_data[x, y] = (255, 0, 0, int(a/2))  # red with transparency\n",
    "\n",
    "# Composite\n",
    "result = Image.alpha_composite(img.convert(\"RGBA\"), overlay)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38d7a52e-70a4-4b52-b2d9-f48a179f47e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "openai",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model-sam3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
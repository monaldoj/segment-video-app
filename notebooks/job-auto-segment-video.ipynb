{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9932e4-3836-4923-93b6-ea4a5a5ea7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "white box truck\n",
      "30\n",
      "True\n",
      "<class 'bool'>\n",
      "0.5\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "trigger_location = dbutils.widgets.get(\"trigger_location\")\n",
    "prompt = dbutils.widgets.get(\"prompt\")\n",
    "frame_stride = int(dbutils.widgets.get(\"frame_stride\"))\n",
    "truncate = dbutils.widgets.get(\"truncate\")\n",
    "threshold = float(dbutils.widgets.get(\"threshold\"))\n",
    "\n",
    "# trigger_location = '/Volumes/pubsec_video_processing/cv/auto_segment/images/maren_jack.MOV'\n",
    "# prompt = 'child'\n",
    "# trigger_location = '/Volumes/pubsec_video_processing/cv/auto_segment/inputs/weimaraner-doc-trimmed.mov'\n",
    "# prompt = 'weimaraner'\n",
    "# trigger_location = '/Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov'\n",
    "# prompt = 'white box truck'\n",
    "# frame_stride = 30\n",
    "# truncate = True\n",
    "# threshold = 0.5\n",
    "\n",
    "if truncate==0 or truncate=='false' or truncate=='False':\n",
    "    truncate = False\n",
    "else:\n",
    "    truncate=True\n",
    "\n",
    "print(trigger_location)\n",
    "print(prompt)\n",
    "print(frame_stride)\n",
    "print(truncate)\n",
    "print(type(truncate))\n",
    "print(threshold)\n",
    "print(type(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96eab01a-766e-425a-b371-ddea9aeaad28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.notebook.exit(\"Notebook execution stopped by user request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1e8fd6-64d4-470f-8769-c1aff52982d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_pat = dbutils.secrets.get(\"justinm-buildathon-secrets\", \"hf_pat\")\n",
    "os.environ[\"HF_TOKEN\"] = hf_pat\n",
    "login(token=hf_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976d93b-5b4f-4bc5-b96f-cfe659fd6bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2f72484c754c8bac007f001e9a01f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ded5347793461bb9b8e494ce877c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /local_disk0/user_tmp_data/spark-839a7f37-a4b7-473d-a16c-35/tmpl9_34bfo/python_model.pkl:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Unexpected internal error when monkey patching `Trainer.train`: cannot import name 'HybridCache' from 'transformers' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-839a7f37-a4b7-473d-a16c-35203f2c12bf/lib/python3.12/site-packages/transformers/__init__.py)\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "loading model...\n",
      "loading processor...\n",
      "context loaded\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-sam3-video@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2421fc66-d747-4c14-8f7d-2b659b98cbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import base64\n",
    "import timeit\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4c2931-c8aa-4afc-9c9b-92000c269c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent file: fresno-traffic-cam-white-box-truck.mov\n",
      "Full path: /Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "Modified: 2026-01-12 19:27:23\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if trigger_location.endswith('/') and (trigger_location[-4]!='.' or trigger_location[-5]!='.'):\n",
    "    # Your volume directory path\n",
    "    directory_path = trigger_location\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "    # Filter out directories, keep only files\n",
    "    files = [f for f in files if not f.isDir()]\n",
    "\n",
    "    # Sort by modification time (most recent first)\n",
    "    files_sorted = sorted(files, key=lambda x: x.modificationTime, reverse=True)\n",
    "\n",
    "    # Get the most recent file\n",
    "    if files_sorted:\n",
    "        most_recent_file = files_sorted[0]\n",
    "        most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "        most_recent_name = most_recent_file.name\n",
    "        \n",
    "        print(f\"Most recent file: {most_recent_name}\")\n",
    "        print(f\"Full path: {most_recent_path}\")\n",
    "        print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")\n",
    "    else:\n",
    "        print(\"No files found in directory\")\n",
    "else:\n",
    "    most_recent_file = dbutils.fs.ls(trigger_location)[0]\n",
    "    most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "    most_recent_name = most_recent_file.name\n",
    "\n",
    "    print(f\"Most recent file: {most_recent_name}\")\n",
    "    print(f\"Full path: {most_recent_path}\")\n",
    "    print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da64b5b-c1db-4192-adfd-7bb299d80533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most_recent_file = trigger_location\n",
    "# most_recent_name = most_recent_file.split(\"/\")[-1]\n",
    "# most_recent_path = most_recent_file.replace('dbfs:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89a223c-86cd-4e22-bbab-ed4caa531bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_results(FILE_URL, results, truncate=True, frame_stride=30, speed_up=5):\n",
    "    import os\n",
    "\n",
    "    OUTPUT_FILE_URL = FILE_URL.replace(\"inputs\", \"outputs\")\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    FPS = 30\n",
    "\n",
    "    def decode_mask(encoded_mask: str) -> np.ndarray:\n",
    "        \"\"\"Decode base64 mask back to numpy array\"\"\"\n",
    "        buf = BytesIO(base64.b64decode(encoded_mask))\n",
    "        return np.load(buf)\n",
    "\n",
    "    def add_text_overlays(frame: np.ndarray, timestamp_sec: float, count_text: str) -> np.ndarray:\n",
    "        \"\"\"Add timestamp overlay to frame in the top-right corner\"\"\"\n",
    "        # Convert seconds to HH:MM:SS.ms format\n",
    "        hours = int(timestamp_sec // 3600)\n",
    "        minutes = int((timestamp_sec % 3600) // 60)\n",
    "        seconds = int(timestamp_sec % 60)\n",
    "        milliseconds = int((timestamp_sec % 1) * 1000)\n",
    "            \n",
    "        timestamp_text = f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        frame_with_overlay = frame.copy()\n",
    "        \n",
    "        # Get frame dimensions\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Set up text properties\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1.5\n",
    "        font_thickness = 2\n",
    "        text_color = (255, 255, 255)  # White text\n",
    "        bg_color = (0, 0, 0)  # Black background\n",
    "        padding = 10\n",
    "        \n",
    "        # Get text size\n",
    "        (text_width_right, text_height_right), baseline_right = cv2.getTextSize(\n",
    "            timestamp_text, font, font_scale, font_thickness\n",
    "        )\n",
    "        \n",
    "        # Position in top-right corner\n",
    "        text_x_right = width - text_width_right - padding\n",
    "        text_y_right = padding + text_height_right\n",
    "        \n",
    "        # Draw semi-transparent background rectangle\n",
    "        bg_x1_right = text_x_right - 5\n",
    "        bg_y1_right = text_y_right - text_height_right - 5\n",
    "        bg_x2_right = text_x_right + text_width_right + 5\n",
    "        bg_y2_right = text_y_right + baseline_right + 5\n",
    "\n",
    "        # Get text size\n",
    "        (text_width_left, text_height_left), baseline_left = cv2.getTextSize(\n",
    "            count_text, font, font_scale, font_thickness\n",
    "        )\n",
    "\n",
    "        # Position in top-left corner\n",
    "        text_x_left = padding\n",
    "        text_y_left = padding + text_height_left\n",
    "        \n",
    "        # Draw semi-transparent background rectangle\n",
    "        bg_x1_left = text_x_left - 5\n",
    "        bg_y1_left = text_y_left - text_height_left - 5\n",
    "        bg_x2_left = text_x_left + text_width_left + 5\n",
    "        bg_y2_left = text_y_left + baseline_left + 5\n",
    "\n",
    "        # Create overlay for semi-transparency\n",
    "        overlay = frame_with_overlay.copy()\n",
    "        cv2.rectangle(overlay, (bg_x1_right, bg_y1_right), (bg_x2_right, bg_y2_right), bg_color, -1)\n",
    "        cv2.rectangle(overlay, (bg_x1_left, bg_y1_left), (bg_x2_left, bg_y2_left), bg_color, -1)\n",
    "        cv2.addWeighted(overlay, 0.6, frame_with_overlay, 0.4, 0, frame_with_overlay)\n",
    "        \n",
    "        # Draw text\n",
    "        cv2.putText(\n",
    "            frame_with_overlay,\n",
    "            timestamp_text,\n",
    "            (text_x_right, text_y_right),\n",
    "            font,\n",
    "            font_scale,\n",
    "            text_color,\n",
    "            font_thickness,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        # Draw text\n",
    "        cv2.putText(\n",
    "            frame_with_overlay,\n",
    "            count_text,\n",
    "            (text_x_left, text_y_left),\n",
    "            font,\n",
    "            font_scale,\n",
    "            text_color,\n",
    "            font_thickness,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        return frame_with_overlay\n",
    "\n",
    "    # Detect input video format and setup codec\n",
    "    import os\n",
    "    _, input_ext = os.path.splitext(FILE_URL)\n",
    "    _, output_ext = os.path.splitext(OUTPUT_FILE_URL)\n",
    "    \n",
    "    # Use the same extension as input if output doesn't have one\n",
    "    if not output_ext:\n",
    "        OUTPUT_FILE_URL = OUTPUT_FILE_URL + input_ext\n",
    "        output_ext = input_ext\n",
    "    \n",
    "    print(f\"Input format: {input_ext}, Output format: {output_ext}\")\n",
    "    \n",
    "    # Open original video to get frames\n",
    "    print(\"Processing frames and applying masks...\")\n",
    "    cap = cv2.VideoCapture(FILE_URL)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or FPS\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Video properties: {frame_width}x{frame_height} @ {fps} fps\")\n",
    "\n",
    "    # Create a mapping of frame_idx to results\n",
    "    print(f\"Found {len(results)} frames in results\")\n",
    "    result_map = {r[\"frame_idx\"]: r for r in results}\n",
    "    # print(\"keys\", result_map.keys())\n",
    "\n",
    "    # Initialize imageio writer for direct frame-by-frame writing with H.264\n",
    "    import imageio\n",
    "    import tempfile\n",
    "    with tempfile.NamedTemporaryFile(suffix=output_ext, delete=False) as tmp_file:\n",
    "        temp_video_path = tmp_file.name\n",
    "    \n",
    "    output_fps = (fps/frame_stride) * speed_up if frame_stride is not None else fps * speed_up\n",
    "    \n",
    "    # Use imageio's get_writer for frame-by-frame writing with H.264 (via bundled ffmpeg)\n",
    "    # This automatically uses imageio-ffmpeg which bundles ffmpeg binaries\n",
    "    video_writer = imageio.get_writer(\n",
    "        temp_video_path,\n",
    "        fps=output_fps,\n",
    "        codec='libx264',\n",
    "        pixelformat='yuv420p',\n",
    "        quality=8  # Quality: 0 (worst) to 10 (best)\n",
    "    )\n",
    "    \n",
    "    print(f\"Writing video at {output_fps:.2f} fps using imageio with H.264 codec...\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    saved_images = []\n",
    "    frames_written = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Only process frames with segmentation results\n",
    "        if frame_idx in result_map:\n",
    "            # print(frame_idx)\n",
    "            # Convert BGR to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            res = result_map[frame_idx]\n",
    "\n",
    "            if res[\"masks\"]:\n",
    "                # Get all of the masks\n",
    "                masks = [decode_mask(x) for x in res[\"masks\"]]\n",
    "                no_of_masks = len(masks)\n",
    "                \n",
    "                # Overlay with transparency\n",
    "                overlay = rgb_frame.copy()\n",
    "                for mask in masks:\n",
    "                    overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "                masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "                                \n",
    "            else:\n",
    "                masked_frame = rgb_frame\n",
    "                no_of_masks = 0\n",
    "                        \n",
    "            # Calculate timestamp for this frame\n",
    "            timestamp_sec = frame_idx / fps\n",
    "            \n",
    "            # Add timestamp overlay (convert back to BGR for cv2 operations, then back to RGB)\n",
    "            masked_frame_bgr = cv2.cvtColor(masked_frame, cv2.COLOR_RGB2BGR)\n",
    "            # masked_frame_with_timestamp = add_timestamp(masked_frame_bgr, timestamp_sec)\n",
    "            # masked_frame_with_timestamp_and_count = add_text_overlay(masked_frame_with_timestamp, f\"Count: {len(res['masks'])}\")\n",
    "            masked_frame_with_text_overlays = add_text_overlays(masked_frame_bgr, timestamp_sec, f\"Count: {no_of_masks}\")\n",
    "            masked_frame = cv2.cvtColor(masked_frame_with_text_overlays, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Save original frame to saved_images\n",
    "            if not truncate:\n",
    "                saved_images.append(Image.fromarray(rgb_frame))\n",
    "                # Write segmented frame directly to video using imageio (expects RGB)\n",
    "                video_writer.append_data(masked_frame)\n",
    "                frames_written += 1\n",
    "            elif res[\"masks\"]:\n",
    "                saved_images.append(Image.fromarray(rgb_frame))\n",
    "                # Write segmented frame directly to video using imageio (expects RGB)\n",
    "                video_writer.append_data(masked_frame)\n",
    "                frames_written += 1\n",
    "            \n",
    "            # if frames_written % 100 == 0:\n",
    "            #     print(f\"Written {frames_written} frames\")\n",
    "            \n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.close()  # Close imageio writer\n",
    "    \n",
    "    print(f\"Saved {len(saved_images)} frames to memory\")\n",
    "    print(f\"Written {frames_written} frames to video with H.264 encoding\")\n",
    "\n",
    "    # 3. Copy video to final destination\n",
    "    import shutil\n",
    "    \n",
    "    temp_size = os.path.getsize(temp_video_path)\n",
    "    print(f\"Video created with H.264: {temp_size:,} bytes ({temp_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "    # Copy to final destination\n",
    "    print(f\"Copying to Volumes: {OUTPUT_FILE_URL}\")\n",
    "    shutil.copy2(temp_video_path, OUTPUT_FILE_URL)\n",
    "\n",
    "    final_size = os.path.getsize(OUTPUT_FILE_URL)\n",
    "    print(f\"✓ Video successfully saved to: {OUTPUT_FILE_URL}\")\n",
    "    print(f\"  Final size: {final_size:,} bytes ({final_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_video_path):\n",
    "        os.remove(temp_video_path)\n",
    "        print(\"Cleaned up temporary file\")\n",
    "\n",
    "    return saved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2eb059-b15b-4f7e-99b6-93ed365bbe38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_file(triggered_file, prompt, resize=True):\n",
    "    print(f\"Triggered by file: {triggered_file}\")\n",
    "\n",
    "    model_input = pd.DataFrame([{\n",
    "        \"video_path\": triggered_file,\n",
    "        \"prompt\": prompt,\n",
    "        \"frame_stride\": frame_stride,  # Process every nth frame\n",
    "        \"batch_size\": 32,\n",
    "        \"threshold\": threshold,\n",
    "        \"mask_threshold\": 0.5\n",
    "    }])\n",
    "    results = model.predict(model_input)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fe1022-307b-4535-b851-ebb432a377f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed in this run: /Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "Segmenting video...\n",
      "Triggered by file: /Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "File to process: /Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "Video path: /Volumes/pubsec_video_processing/cv/auto_segment/inputs/fresno-traffic-cam-white-box-truck.mov\n",
      "Prompt: white box truck\n",
      "Frame stride: 30\n",
      "Batch size: 32\n",
      "Threshold: 0.5\n",
      "Mask threshold: 0.5\n",
      "Inference time: 858 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Files processed in this run:\", most_recent_path)\n",
    "\n",
    "print(\"Segmenting video...\")\n",
    "starting_time = timeit.default_timer()\n",
    "results = process_file(most_recent_path, prompt, resize=False)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time))} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90cc529-6045-43da-90a0-d7edbfad0c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ace2a9e-5bfd-436a-b47d-59282c459e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_path = \"/Volumes/pubsec_video_processing/cv/auto_segment/images/results.pkl\"\n",
    "# with open(pickle_path, \"wb\") as f:\n",
    "#     pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99e65f1-73a0-4b45-aae1-cf46d43ba34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# pickle_path = \"/Volumes/pubsec_video_processing/cv/auto_segment/images/results.pkl\"\n",
    "# with open(pickle_path, \"rb\") as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22671e7f-aeec-4283-8ea1-4dd5c331d4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing segmented video...\n",
      "Input format: .mov, Output format: .mov\n",
      "Processing frames and applying masks...\n",
      "Video properties: 1680x942 @ 59.92300129366106 fps\n",
      "Found 3217 frames in results\n",
      "Writing video at 9.99 fps using imageio with H.264 codec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1680, 942) to (1680, 944) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 100 frames\n",
      "Written 200 frames\n",
      "Written 300 frames\n",
      "Written 400 frames\n",
      "Written 400 frames\n",
      "Written 500 frames\n",
      "Written 600 frames\n",
      "Written 700 frames\n",
      "Written 800 frames\n",
      "Written 900 frames\n",
      "Written 1000 frames\n",
      "Written 1100 frames\n",
      "Saved 1158 frames to memory\n",
      "Written 1158 frames to video with H.264 encoding\n",
      "Video created with H.264: 337,471,413 bytes (321.84 MB)\n",
      "Copying to Volumes: /Volumes/pubsec_video_processing/cv/auto_segment/outputs/fresno-traffic-cam-white-box-truck.mov\n",
      "✓ Video successfully saved to: /Volumes/pubsec_video_processing/cv/auto_segment/outputs/fresno-traffic-cam-white-box-truck.mov\n",
      "  Final size: 337,471,413 bytes (321.84 MB)\n",
      "Cleaned up temporary file\n",
      "Writing time: 249 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing segmented video...\")\n",
    "starting_time = timeit.default_timer()\n",
    "written_images = write_results(most_recent_path, results, truncate=truncate, frame_stride=frame_stride, speed_up=5)\n",
    "print(f\"Writing time: {round((timeit.default_timer() - starting_time))} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57647b7b-b98d-4d04-8126-2f132f1ffdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde6b7408687406d93851a2b32d01d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bee20398094d2f99e52e9da31da31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /local_disk0/user_tmp_data/spark-4e29edd4-f093-45cf-bad3-47/tmpk81qljjv/python_model.pkl:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: vdm-catalog-7rnpbv-ext-s3-049629455384-ph2i0s.s3.amazonaws.com. Connection pool size: 10\n",
      "/databricks/python/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "Unexpected internal error when monkey patching `Trainer.train`: cannot import name 'HybridCache' from 'transformers' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-4e29edd4-f093-45cf-bad3-4798f385244f/lib/python3.12/site-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bb1949cd344453be6c3fc3e31ab9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `BlipImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d5bec697ac47658120700d4e8fbaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62bd4b46a894997a96e08413a87920c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219f11311ddf4c06a8c197bf5e409d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2bd6eb28464d05b2b85375e35b9531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6855bcd10d6249d7896ae56065200b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5812fec8eb345059e606a6110932c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e0e0fe5e2b4ebab2fd2137b8919332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "BlipForConditionalGeneration LOAD REPORT from: Salesforce/blip-image-captioning-large\n",
      "Key                                       | Status     |  | \n",
      "------------------------------------------+------------+--+-\n",
      "text_decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "captioning_model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-blip@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed8dcfa-be29-446c-b634-ce4fb9272764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL.Image.Image\n",
    "        format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 string that can be safely passed in JSON\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=format)\n",
    "    buf.seek(0)\n",
    "    b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return b64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239588f-5b93-4e7e-afe9-9912078ee025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "written_images_str = [pil_to_base64_str(x) for x in written_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a37d75f-a512-4509-a011-65f71d0e0663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_input = {\n",
    "    \"image_path\": written_images_str\n",
    "}\n",
    "\n",
    "captions = captioning_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e8f28-0261-4bc9-8130-e5875c1c7bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Functional Narrative Summary (Over Time)\n",
      "\n",
      "The video alternates between two main settings: an organized **dog show** and more casual **outdoor play by fields and water**. It moves back and forth, but the overall progression is from **handling and evaluation** to **active retrieving and water play**, with recurring cutaways to other animals near the same waterside environment.\n",
      "\n",
      "### 1) Dog show handling and judging\n",
      "A woman—often in a **red dress**—is shown repeatedly **petting and presenting dogs on leashes** in a show environment. Dogs are walked in a **ring** or along a **stage**, guided carefully to display posture and movement. At moments, the woman and a man in a **tan/beige suit** perform more formal checks: the dog is **examined up close**, including a sequence where a **stethoscope is used** and another where the dog’s **teeth/mouth are inspected**, consistent with judging or veterinary-style evaluation. Grooming appears as part of preparation: a dog is **brushed/handled** while standing, then returned to leash control and positioned again for assessment. Occasionally, two handlers walk a dog together, reinforcing the structured, supervised tone of the show segments.\n",
      "\n",
      "### 2) Transition to relaxed companionship in parks\n",
      "Between show clips, the mood shifts to a park-like setting where a smiling woman (often in a **black shirt**) **holds a dog in her arms** and poses or stands with it among trees. She also sits on grass with one or two dogs, signaling a calmer interlude where the dogs are simply **kept close, petted, and comforted** rather than formally displayed.\n",
      "\n",
      "### 3) Frisbee retrieving in open fields\n",
      "The action becomes more dynamic as dogs begin **retrieving frisbees**. Dogs sprint across grass and dirt with a **frisbee clamped in their mouths**, sometimes running toward the camera or alongside a person. A woman is shown **playing frisbee**—holding it, throwing or preparing to throw—while dogs repeatedly **fetch and return**. At points, **two dogs run together**, sharing the space and chasing in parallel, emphasizing ongoing play cycles rather than a single throw.\n",
      "\n",
      "### 4) Fetch substitutes: sticks, bottles, and unusual “catches”\n",
      "Retrieval behavior expands beyond frisbees: dogs carry **sticks** while moving through grass, dirt, and near shorelines. One sequence shows a dog standing in shallow water holding a **bottle**, then similar waterside shots show dogs holding **sticks**. There are also odd, attention-grabbing cuts where animals are depicted with unexpected items—such as a dog with a **fish in its mouth**—suggesting either comedic montage or rapid intercutting of varied “carry” moments.\n",
      "\n",
      "### 5) Waterline activity: wading, swimming, and shoreline play\n",
      "The setting repeatedly returns to a lake/river edge. Dogs **wade in shallow water**, stand near people, and then **swim** while holding objects (frisbees, sticks, and in one instance a fish-like item). Some shots show a dog sitting low in the water by the shore, then later actively paddling farther out. A woman occasionally stands on rocks or at the shoreline with a dog, watching or encouraging the water retrieves, while other clips show dogs walking along the water’s edge still carrying their retrieved items.\n",
      "\n",
      "### 6) Cutaways to other animals near the water\n",
      "Interspersed with the dog-focused action are brief, abrupt cutaways: **ducks** and **birds** floating or standing in shallow water, sometimes humorously described as carrying a frisbee or fish; and more surreal inserts of larger animals like a **bear** (wading, swimming, floating) and even an **elephant** in water holding or near objects. These appear as montage-style interruptions that mirror the dogs’ “in-water carrying” behavior, then the video returns to dogs retrieving and handlers interacting.\n",
      "\n",
      "### 7) Closing rhythm: continued alternating between show control and outdoor play\n",
      "The latter portion continues the established pattern: quick returns to the **dog show** for more leash walking, petting, grooming, and examination, followed by more **field running** and **water retrieves**. The functional throughline remains consistent—humans direct and handle dogs in formal contexts, then dogs repeatedly perform **fetch-and-carry** behaviors in open environments, especially along the shoreline.\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "\n",
    "# Initialize the client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Define your endpoint name\n",
    "endpoint_name = \"databricks-gpt-5-2\"\n",
    "\n",
    "# # Create a chat message\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. Can you identify anything out of the ordinary or anomalous in the video? When I say anomlous, I mean actions taking place that are against the law or violent in nature. If you find something that shouldn't be happening, describe why it is anomolous. If you don't find anything abnormal, just respond with 'Nothing anomolous found.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "# Create a chat message\n",
    "captions = list(set(captions))\n",
    "prompt = f\"\"\"Summarize the following sections of a video with a focus on what is functionally happening over time. Be descriptive. The resulting summary should be a functional narrative. Return this as Markdown text - and only markdown. \\n\\n {captions}\"\"\"\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. There may be obvious scene changes based on the descriptions. Please summarize the contents of each scene in the video in a single sentence each.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=ChatMessageRole.USER,\n",
    "        content=prompt\n",
    "    )\n",
    "]\n",
    "\n",
    "# Query the endpoint\n",
    "response = w.serving_endpoints.query(\n",
    "    name=endpoint_name,\n",
    "    messages=messages,\n",
    "    # max_tokens=500  # optional parameter\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "r = response.choices[0].message.content\n",
    "# print(r)\n",
    "try:\n",
    "    text = [x for x in r if x['type'] == 'text'][0]['text']\n",
    "except:\n",
    "    text = r\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cf504d-be11-4eca-a71e-55b12c120b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/pubsec_video_processing/cv/auto_segment/descriptions/weimaraner-doc-trimmed.txt\n",
      "Wrote 4316 bytes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "print(txt_filename)\n",
    "\n",
    "dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc51046-5099-4b11-97cc-1be7bb9771f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da14035e-1a7f-48c5-936f-93bd12fe1ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "databricks_ai_v4",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python==4.12.0.88",
     "accelerate==1.12.0"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "job-auto-segment-video",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

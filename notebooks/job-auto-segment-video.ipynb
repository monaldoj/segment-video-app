{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9932e4-3836-4923-93b6-ea4a5a5ea7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trigger_location = dbutils.widgets.get(\"trigger_location\")\n",
    "prompt = dbutils.widgets.get(\"prompt\")\n",
    "frame_stride = int(dbutils.widgets.get(\"frame_stride\"))\n",
    "truncate = dbutils.widgets.get(\"truncate\")\n",
    "threshold = float(dbutils.widgets.get(\"threshold\"))\n",
    "\n",
    "# trigger_location = '/Volumes/pubsec_video_processing/cv/auto_segment/images/maren_jack.MOV'\n",
    "# prompt = 'girl in pink sweatshrit'\n",
    "# frame_stride = 30\n",
    "# truncate = True\n",
    "# threshold = 0.2\n",
    "\n",
    "if truncate==0 or truncate=='false' or truncate=='False':\n",
    "  truncate = False\n",
    "else:\n",
    "  truncate=True\n",
    "\n",
    "print(trigger_location)\n",
    "print(prompt)\n",
    "print(frame_stride)\n",
    "print(truncate)\n",
    "print(type(truncate))\n",
    "print(threshold)\n",
    "print(type(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96eab01a-766e-425a-b371-ddea9aeaad28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.notebook.exit(\"Notebook execution stopped by user request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2421fc66-d747-4c14-8f7d-2b659b98cbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import base64\n",
    "import timeit\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4c2931-c8aa-4afc-9c9b-92000c269c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if trigger_location.endswith('/') and (trigger_location[-4]!='.' or trigger_location[-5]!='.'):\n",
    "  # Your volume directory path\n",
    "  directory_path = trigger_location\n",
    "\n",
    "  # List all files in the directory\n",
    "  files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "  # Filter out directories, keep only files\n",
    "  files = [f for f in files if not f.isDir()]\n",
    "\n",
    "  # Sort by modification time (most recent first)\n",
    "  files_sorted = sorted(files, key=lambda x: x.modificationTime, reverse=True)\n",
    "\n",
    "  # Get the most recent file\n",
    "  if files_sorted:\n",
    "      most_recent_file = files_sorted[0]\n",
    "      most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "      most_recent_name = most_recent_file.name\n",
    "      \n",
    "      print(f\"Most recent file: {most_recent_name}\")\n",
    "      print(f\"Full path: {most_recent_path}\")\n",
    "      print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")\n",
    "  else:\n",
    "      print(\"No files found in directory\")\n",
    "else:\n",
    "  most_recent_file = dbutils.fs.ls(trigger_location)[0]\n",
    "  most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "  most_recent_name = most_recent_file.name\n",
    "\n",
    "  print(f\"Most recent file: {most_recent_name}\")\n",
    "  print(f\"Full path: {most_recent_path}\")\n",
    "  print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da64b5b-c1db-4192-adfd-7bb299d80533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most_recent_file = trigger_location\n",
    "# most_recent_name = most_recent_file.split(\"/\")[-1]\n",
    "# most_recent_path = most_recent_file.replace('dbfs:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1e8fd6-64d4-470f-8769-c1aff52982d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_pat = dbutils.secrets.get(\"justinm-buildathon-secrets\", \"hf_pat\")\n",
    "os.environ[\"HF_TOKEN\"] = hf_pat\n",
    "login(token=hf_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976d93b-5b4f-4bc5-b96f-cfe659fd6bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-sam3-video@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbde7081-11ad-4b1c-b192-92766db808ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_results(FILE_URL, results):\n",
    "  import os\n",
    "\n",
    "  OUTPUT_FILE_URL = FILE_URL.replace(\"inputs\", \"outputs\")\n",
    "  output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  FPS = 24\n",
    "\n",
    "  def decode_mask(encoded_mask: str) -> np.ndarray:\n",
    "      \"\"\"Decode base64 mask back to numpy array\"\"\"\n",
    "      buf = BytesIO(base64.b64decode(encoded_mask))\n",
    "      return np.load(buf)\n",
    "\n",
    "  def add_timestamp(frame: np.ndarray, timestamp_sec: float) -> np.ndarray:\n",
    "      \"\"\"Add timestamp overlay to frame in the top-right corner\"\"\"\n",
    "      # Convert seconds to HH:MM:SS.ms format\n",
    "      hours = int(timestamp_sec // 3600)\n",
    "      minutes = int((timestamp_sec % 3600) // 60)\n",
    "      seconds = int(timestamp_sec % 60)\n",
    "      milliseconds = int((timestamp_sec % 1) * 1000)\n",
    "      \n",
    "      timestamp_text = f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "      \n",
    "      # Create a copy to avoid modifying original\n",
    "      frame_with_timestamp = frame.copy()\n",
    "      \n",
    "      # Get frame dimensions\n",
    "      height, width = frame.shape[:2]\n",
    "      \n",
    "      # Set up text properties\n",
    "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "      font_scale = 0.7\n",
    "      font_thickness = 2\n",
    "      text_color = (255, 255, 255)  # White text\n",
    "      bg_color = (0, 0, 0)  # Black background\n",
    "      padding = 10\n",
    "      \n",
    "      # Get text size\n",
    "      (text_width, text_height), baseline = cv2.getTextSize(\n",
    "          timestamp_text, font, font_scale, font_thickness\n",
    "      )\n",
    "      \n",
    "      # Position in top-right corner\n",
    "      text_x = width - text_width - padding\n",
    "      text_y = padding + text_height\n",
    "      \n",
    "      # Draw semi-transparent background rectangle\n",
    "      bg_x1 = text_x - 5\n",
    "      bg_y1 = text_y - text_height - 5\n",
    "      bg_x2 = text_x + text_width + 5\n",
    "      bg_y2 = text_y + baseline + 5\n",
    "      \n",
    "      # Create overlay for semi-transparency\n",
    "      overlay = frame_with_timestamp.copy()\n",
    "      cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), bg_color, -1)\n",
    "      cv2.addWeighted(overlay, 0.6, frame_with_timestamp, 0.4, 0, frame_with_timestamp)\n",
    "      \n",
    "      # Draw text\n",
    "      cv2.putText(\n",
    "          frame_with_timestamp,\n",
    "          timestamp_text,\n",
    "          (text_x, text_y),\n",
    "          font,\n",
    "          font_scale,\n",
    "          text_color,\n",
    "          font_thickness,\n",
    "          cv2.LINE_AA\n",
    "      )\n",
    "      \n",
    "      return frame_with_timestamp\n",
    "\n",
    "  # Open original video to get frames\n",
    "  print(\"Processing frames and applying masks...\")\n",
    "  cap = cv2.VideoCapture(FILE_URL)\n",
    "  fps = cap.get(cv2.CAP_PROP_FPS) or FPS\n",
    "\n",
    "  # Create a mapping of frame_idx to results\n",
    "  print(f\"Found {len(results)} frames in results\")\n",
    "  result_map = {r[\"frame_idx\"]: r for r in results}\n",
    "\n",
    "  frame_idx = 0\n",
    "  saved_images = []\n",
    "  segmented_images = []\n",
    "\n",
    "  while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "          break\n",
    "      \n",
    "      # Only process frames with segmentation results\n",
    "      if frame_idx in result_map:\n",
    "          # Convert BGR to RGB\n",
    "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "          \n",
    "          res = result_map[frame_idx]\n",
    "          \n",
    "          # if res[\"masks\"]:\n",
    "          #     # Get the first (highest score) mask\n",
    "          #     mask = decode_mask(res[\"masks\"][0])\n",
    "              \n",
    "          #     # Overlay with transparency\n",
    "          #     overlay = rgb_frame.copy()\n",
    "          #     overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "          #     masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "\n",
    "          if res[\"masks\"]:\n",
    "              # Get the first (highest score) mask\n",
    "              masks = [decode_mask(x) for x in res[\"masks\"]]\n",
    "              \n",
    "              # Overlay with transparency\n",
    "              overlay = rgb_frame.copy()\n",
    "              for mask in masks:\n",
    "                  overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "              masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "              \n",
    "          else:\n",
    "              masked_frame = rgb_frame\n",
    "          \n",
    "          # Calculate timestamp for this frame\n",
    "          timestamp_sec = frame_idx / fps\n",
    "          \n",
    "          # Add timestamp overlay (convert back to BGR for cv2 operations, then back to RGB)\n",
    "          masked_frame_bgr = cv2.cvtColor(masked_frame, cv2.COLOR_RGB2BGR)\n",
    "          masked_frame_with_timestamp = add_timestamp(masked_frame_bgr, timestamp_sec)\n",
    "          masked_frame = cv2.cvtColor(masked_frame_with_timestamp, cv2.COLOR_BGR2RGB)\n",
    "          \n",
    "          # Save frame\n",
    "          if not truncate:\n",
    "              saved_images.append(Image.fromarray(rgb_frame))\n",
    "              segmented_images.append(Image.fromarray(masked_frame))\n",
    "          elif res[\"masks\"]:\n",
    "              saved_images.append(Image.fromarray(rgb_frame))\n",
    "              segmented_images.append(Image.fromarray(masked_frame))\n",
    "      \n",
    "      frame_idx += 1\n",
    "\n",
    "  cap.release()\n",
    "  print(f\"Saved {len(saved_images)} frames to memory\")\n",
    "  print(f\"Saved {len(segmented_images)} segmented frames to memory\")\n",
    "\n",
    "  # 3. Create full segmented video\n",
    "  import imageio\n",
    "  import os\n",
    "  import shutil\n",
    "  import tempfile\n",
    "\n",
    "  print(\"Writing video to temporary file...\")\n",
    "  with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:\n",
    "      temp_video_path = tmp_file.name\n",
    "\n",
    "  imageio.mimsave(\n",
    "      temp_video_path,\n",
    "      segmented_images,\n",
    "      fps=(fps/frame_stride) * 5,  # Use original FPS instead of hardcoded 24\n",
    "      codec='libx264',\n",
    "      pixelformat='yuv420p'\n",
    "  )\n",
    "\n",
    "  temp_size = os.path.getsize(temp_video_path)\n",
    "  print(f\"Temporary video created: {temp_size:,} bytes ({temp_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "  # Copy to Volumes\n",
    "  print(f\"Copying to Volumes: {OUTPUT_FILE_URL}\")\n",
    "  shutil.copy2(temp_video_path, OUTPUT_FILE_URL)\n",
    "\n",
    "  final_size = os.path.getsize(OUTPUT_FILE_URL)\n",
    "  print(f\"âœ“ Video successfully saved to: {OUTPUT_FILE_URL}\")\n",
    "  print(f\"  Final size: {final_size:,} bytes ({final_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "  # Clean up temporary file\n",
    "  if os.path.exists(temp_video_path):\n",
    "      os.remove(temp_video_path)\n",
    "      print(\"Cleaned up temporary file\")\n",
    "\n",
    "  return saved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2eb059-b15b-4f7e-99b6-93ed365bbe38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_file(triggered_file, prompt):\n",
    "    print(f\"Triggered by file: {triggered_file}\")\n",
    "    # Your processing logic here\n",
    "    model_input = pd.DataFrame([{\n",
    "        \"video_path\": triggered_file,\n",
    "        \"prompt\": prompt,\n",
    "        \"frame_stride\": frame_stride,  # Process every nth frame\n",
    "        \"batch_size\": 16,\n",
    "        \"threshold\": threshold,\n",
    "        \"mask_threshold\": 0.5\n",
    "    }])\n",
    "    results = model.predict(model_input)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fe1022-307b-4535-b851-ebb432a377f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Files processed in this run:\", most_recent_path)\n",
    "\n",
    "print(\"Segmenting video...\")\n",
    "starting_time = timeit.default_timer()\n",
    "results = process_file(most_recent_path, prompt)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time))} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22671e7f-aeec-4283-8ea1-4dd5c331d4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "written_images = write_results(most_recent_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57647b7b-b98d-4d04-8126-2f132f1ffdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "captioning_model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-blip@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed8dcfa-be29-446c-b634-ce4fb9272764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL.Image.Image\n",
    "        format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 string that can be safely passed in JSON\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=format)\n",
    "    buf.seek(0)\n",
    "    b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return b64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239588f-5b93-4e7e-afe9-9912078ee025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "written_images_str = [pil_to_base64_str(x) for x in written_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a37d75f-a512-4509-a011-65f71d0e0663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_input = {\n",
    "    \"image_path\": written_images_str\n",
    "}\n",
    "\n",
    "captions = captioning_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e8f28-0261-4bc9-8130-e5875c1c7bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;36m  File \u001b[0;32m<command-7680324554620704>, line 19\u001b[0;36m\u001b[0m\n",
       "\u001b[0;31m    'Summarize the following sections of a video with a focus on '\u001b[0m\n",
       "\u001b[0m    ^\u001b[0m\n",
       "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "IndentationError",
        "evalue": "unexpected indent (command-7680324554620704-1709804000, line 19)"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "USER_CODE_SYNTAX_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN02",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;36m  File \u001b[0;32m<command-7680324554620704>, line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    'Summarize the following sections of a video with a focus on '\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "\n",
    "# Initialize the client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Define your endpoint name\n",
    "endpoint_name = \"databricks-gpt-5-2\"\n",
    "\n",
    "# # Create a chat message\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. Can you identify anything out of the ordinary or anomalous in the video? When I say anomlous, I mean actions taking place that are against the law or violent in nature. If you find something that shouldn't be happening, describe why it is anomolous. If you don't find anything abnormal, just respond with 'Nothing anomolous found.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "# Create a chat message\n",
    "captions = list(set(captions))\n",
    "prompt = f\"\"\"Summarize the following sections of a video with a focus on what is functionally happening over time. Be descriptive. The resulting summary should be a functional narrative. Return this as Markdown text - and only markdown. \\n\\n {captions}\"\"\"\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. There may be obvious scene changes based on the descriptions. Please summarize the contents of each scene in the video in a single sentence each.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=ChatMessageRole.USER,\n",
    "        content=prompt\n",
    "    )\n",
    "]\n",
    "\n",
    "# Query the endpoint\n",
    "response = w.serving_endpoints.query(\n",
    "    name=endpoint_name,\n",
    "    messages=messages,\n",
    "    # max_tokens=500  # optional parameter\n",
    ")\n",
    "\n",
    "# Access the response\n",
    "r = response.choices[0].message.content\n",
    "# print(r)\n",
    "text = [x for x in r if x['type'] == 'text'][0]['text']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cf504d-be11-4eca-a71e-55b12c120b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "print(txt_filename)\n",
    "\n",
    "dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc51046-5099-4b11-97cc-1be7bb9771f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio",
     "imageio[ffmpeg]",
     "imageio[pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "auto-segment-video-job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

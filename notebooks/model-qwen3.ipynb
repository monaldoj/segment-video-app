{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "496ce284-bfed-4144-ad99-74c6d2443d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2599867-09dd-4e9f-8298-5686eceff440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "import io\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "class Qwen3CaptioningModel(PythonModel):\n",
    "    \"\"\"\n",
    "    MLflow PyFunc wrapper for Qwen3-VL-2B-Instruct image captioning model.\n",
    "    \n",
    "    This model accepts images in multiple formats:\n",
    "    - PIL Image objects\n",
    "    - File paths (strings)\n",
    "    - Base64 encoded strings\n",
    "    - Bytes\n",
    "    - URLs\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Load the Qwen3-VL model and processor.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context containing artifacts and parameters\n",
    "        \"\"\"\n",
    "        # Determine device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load processor and model\n",
    "        model_name = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "        \n",
    "        # Load the model\n",
    "        self.model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Load the processor\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        self.model.eval()\n",
    "    \n",
    "    def _convert_image(self, image_path) -> str:\n",
    "        \"\"\"\n",
    "        Convert various image input formats to a format compatible with Qwen3-VL.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Image in various formats (path, bytes, base64, PIL Image, URL)\n",
    "            \n",
    "        Returns:\n",
    "            String representation suitable for Qwen3-VL (file path or URL)\n",
    "        \"\"\"\n",
    "        # print(\"IMAGE PATH:\", image_path)\n",
    "        if isinstance(image_path, Image.Image):\n",
    "            # print(\"IT IS AN IMAGE\")\n",
    "            # Convert PIL Image to base64 data URL\n",
    "            buffered = BytesIO()\n",
    "            image_path.save(buffered, format=\"PNG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        elif isinstance(image_path, str):\n",
    "            # print(\"FIRST LEVEL STRING\")\n",
    "            if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "                # HTTP/HTTPS URL - return as is\n",
    "                # print(\"IT IS AN HTTP\")\n",
    "                return image_path\n",
    "            elif image_path.startswith('/') or image_path.startswith('file://'):\n",
    "                # Local file path\n",
    "                # print(\"IT IS A /\")\n",
    "                if not image_path.startswith('file://'):\n",
    "                    image_path = f\"file://{image_path}\"\n",
    "                return image_path\n",
    "            elif image_path.startswith('data:image'):\n",
    "                # Already a base64 data URL\n",
    "                # print(\"IT IS A base64str\")\n",
    "                return image_path\n",
    "            else:\n",
    "                # Assume it's a base64 string without header\n",
    "                # print(\"IT IS A str to be appended\")\n",
    "                return f\"data:image/png;base64,{image_path}\"\n",
    "        \n",
    "        elif isinstance(image_path, bytes):\n",
    "            # Convert bytes to base64 data URL\n",
    "            # print(\"IT IS bytes\")\n",
    "            img_str = base64.b64encode(image_path).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"âŒ Unsupported image input type: {type(image_path)}\")\n",
    "    \n",
    "    def predict(self, context, model_input, params=None) -> list:\n",
    "        \"\"\"\n",
    "        Generate captions for input images.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context\n",
    "            model_input: Can be a pandas DataFrame with image column, \n",
    "                        a list of images, or a single image\n",
    "            params: Optional parameters dict with:\n",
    "         - max_length: Maximum caption length (default: 128)\n",
    "         - text: Optional conditioning text for guided captioning \n",
    "                                                    (default: \"Describe this image.\")\n",
    "         - top_p: Top-p sampling parameter (default: 0.8)\n",
    "         - top_k: Top-k sampling parameter (default: 20)\n",
    "         - temperature: Temperature for sampling (default: 0.7)\n",
    "         - repetition_penalty: Repetition penalty (default: 1.0)\n",
    "         \n",
    "        Returns:\n",
    "            List of captions\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        \n",
    "        max_length = params.get('max_length', 256)\n",
    "        conditional_text = params.get('text', \"Describe this image.\")\n",
    "        top_p = params.get('top_p', 0.8)\n",
    "        top_k = params.get('top_k', 20)\n",
    "        temperature = params.get('temperature', 0.7)\n",
    "        repetition_penalty = params.get('repetition_penalty', 1.0)\n",
    "        \n",
    "        # Extract image_path from model_input\n",
    "        model_input = model_input['image_path']\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, pd.Series):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, list):\n",
    "            images = model_input\n",
    "        else:\n",
    "            # Single image\n",
    "            images = [model_input]\n",
    "        \n",
    "        # print(\"IMAGES:\", images)\n",
    "        # Convert all images to Qwen-compatible format\n",
    "        image_urls = [self._convert_image(img) for img in images]\n",
    "        \n",
    "        # Generate captions\n",
    "        captions = []\n",
    "        \n",
    "        for image_url in image_urls:\n",
    "            # Construct message for Qwen3-VL\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"image\": image_url,\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": conditional_text},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Prepare for inference\n",
    "            # print(\"MESSAGES:\", messages)\n",
    "            inputs = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = inputs.to(self.device)\n",
    "            \n",
    "            # Generate caption\n",
    "            # print(\"GENERATING CAPTIONS\")\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    temperature=temperature,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    do_sample=True if temperature > 0 else False\n",
    "                )\n",
    "            \n",
    "            # Decode the output\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] \n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            \n",
    "            captions.append(output_text[0])\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04feedfb-388d-4a77-bcc1-239c407aa91d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qwen = Qwen3CaptioningModel()\n",
    "\n",
    "class ContextObject():\n",
    "    def __init__(self, artifacts):\n",
    "    self.artifacts = artifacts\n",
    "\n",
    "artifacts = {}\n",
    "qwen_context = ContextObject(artifacts)\n",
    "\n",
    "qwen.load_context(context = qwen_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc820ff-304c-4830-b4f0-88954d3d160a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_dir = \"/Volumes/pubsec_video_processing/cv/images\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]\n",
    "pil_images = [Image.open(os.path.join(image_dir, f)).convert(\"RGB\") for f in image_files]\n",
    "\n",
    "model_input = {\n",
    "    \"image_path\": pil_images + pil_images + pil_images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35ec47c-580d-4f9b-a55a-db21407d0e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pil_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdf870a5-e4ab-4c95-815b-8bed1ed95bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = qwen.predict(\n",
    "    context = None,\n",
    "    model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6323f9cd-2a55-4e22-8841-e457669046e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return base64_string\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/Volumes/pubsec_video_processing/cv/images/bruno.png\"\n",
    "image_path = image_to_base64(image_path)\n",
    "print(type(image_path))\n",
    "\n",
    "model_input = {\n",
    "    \"image_path\": [image_path]\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = qwen.predict(\n",
    "    context = None,\n",
    "    model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18d79b83-4af2-4c63-bbb1-209e29bbce2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# specify the location the model will be saved/registered in Unity Catalog\n",
    "catalog = \"pubsec_video_processing\"\n",
    "schema = \"cv\"\n",
    "model_name = \"transformers-qwen3-2B-vision\"\n",
    "model_full_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "# mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "signature = infer_signature(model_input=model_input, model_output=response)\n",
    "\n",
    "# Define conda environment with dependencies\n",
    "conda_env = {\n",
    "    'channels': ['conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.12.3',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'mlflow>=2.10.0',\n",
    "                'torch>=2.0.0',\n",
    "                # 'transformers>=4.30.0',\n",
    "                'git+https://github.com/huggingface/transformers.git'\n",
    "                'Pillow',\n",
    "                'torchvision',\n",
    "                \"cloudpickle==3.0.0\",\n",
    "                # 'pillow>=9.0.0',\n",
    "                'numpy>=1.23.0',\n",
    "                'pandas>=1.5.0',\n",
    "                'accelerate>=0.20.0'\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    'name': 'blip_env'\n",
    "}\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=Qwen3CaptioningModel(),\n",
    "        signature=signature,\n",
    "        input_example=model_input,\n",
    "        conda_env=conda_env,\n",
    "        # extra_pip_requirements=[\n",
    "        #   \"torch\",\n",
    "        #   \"git+https://github.com/huggingface/transformers.git\",\n",
    "        #   \"Pillow\"\n",
    "        # ]\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Model registered! URI: runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7abf04ab-ff85-4680-888a-906068847df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce337562-b85b-4112-a29a-264c9e905beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12c079d-1b2f-4880-b01c-5b95500bd57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL.Image.Image\n",
    "        format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 string that can be safely passed in JSON\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=format)\n",
    "    buf.seek(0)\n",
    "    b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return b64_str\n",
    "\n",
    "written_images_base64 = []\n",
    "for pil_img in pil_images + pil_images + pil_images:\n",
    "    written_images_base64.append(pil_to_base64_str(pil_img))\n",
    "\n",
    "model_input = {\n",
    "    \"image_path\": written_images_base64\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d7eec8c-3cb8-4d77-91d7-b9e6c06445d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the model using the \"run\" from above.\n",
    "mlflow.register_model(model_uri=f\"runs:/{run_id}/model\", name=model_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3c7762-ab04-4874-be2d-3300b9d3a5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5ac838-8b4b-465c-ad42-64fb08976fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c924ce25-c4c9-40f8-b209-1b956f1ea7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "openai",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model-qwen3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

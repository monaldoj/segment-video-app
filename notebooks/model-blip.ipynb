{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d548902b-a122-4423-b548-116e3ec46779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "527673c5-b42c-449f-979f-fa9c40d18598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "import io\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "class BLIPCaptioningModel(PythonModel):\n",
    "    \"\"\"\n",
    "    MLflow PyFunc wrapper for Salesforce BLIP image captioning model.\n",
    "    \n",
    "    This model accepts images in multiple formats:\n",
    "    - PIL Image objects\n",
    "    - File paths (strings)\n",
    "    - Base64 encoded strings\n",
    "    - Bytes\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Load the BLIP model and processor.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context containing artifacts and parameters\n",
    "        \"\"\"\n",
    "        # Determine device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load processor and model\n",
    "        model_name = \"Salesforce/blip-image-captioning-large\"\n",
    "        # model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    # def _load_image(self, image_input) -> Image.Image:\n",
    "    #     \"\"\"\n",
    "    #     Convert various image input formats to PIL Image.\n",
    "        \n",
    "    #     Args:\n",
    "    #         image_input: Image in various formats (path, bytes, base64, PIL Image)\n",
    "            \n",
    "    #     Returns:\n",
    "    #         PIL Image object\n",
    "    #     \"\"\"\n",
    "    #     if isinstance(image_input, Image.Image):\n",
    "    #         return image_input\n",
    "    #     elif isinstance(image_input, str):\n",
    "    #         # Check if it's a base64 string\n",
    "    #         if image_input.startswith('data:image'):\n",
    "    #             # Remove data URL prefix\n",
    "    #             base64_str = image_input.split(',')[1]\n",
    "    #             image_bytes = base64.b64decode(base64_str)\n",
    "    #             return Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "    #         elif image_input.startswith('base64:'):\n",
    "    #             # Custom base64 prefix\n",
    "    #             base64_str = image_input[7:]\n",
    "    #             image_bytes = base64.b64decode(base64_str)\n",
    "    #             return Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "    #         else:\n",
    "    #             # Assume it's a file path\n",
    "    #             return Image.open(image_input).convert('RGB')\n",
    "    #     elif isinstance(image_input, bytes):\n",
    "    #         return Image.open(io.BytesIO(image_input)).convert('RGB')\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Unsupported image input type: {type(image_input)}\")\n",
    "    \n",
    "    def _convert_image(self, image_path) -> Image.Image:\n",
    "        if isinstance(image_path, Image.Image):\n",
    "            print(\"IT IS AN IMAGE\")\n",
    "            image = image_path\n",
    "        elif image_path.startswith(\"http\"):\n",
    "            print(\"IT IS AN HTTP\")\n",
    "            image = Image.open(requests.get(image_path, stream=True).raw).convert(\"RGB\")\n",
    "        elif image_path.startswith('/'):\n",
    "            print(\"IT IS A /\")\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        elif image_path.startswith('data:image'):\n",
    "            print(\"IT IS A data:image\")\n",
    "            header, b64_string = image_path.split(\",\", 1)\n",
    "            image_bytes = base64.b64decode(b64_string)\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        elif isinstance(image_path, str):\n",
    "            print(\"IT IS A str\")\n",
    "            image_bytes = base64.b64decode(image_path)\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        elif isinstance(image_path, bytes):\n",
    "            print(\"IT IS bytes\")\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "        else:\n",
    "            # return \"❌ Unsupported image type\"\n",
    "            raise ValueError(f\"❌ Unsupported image input type: {type(image_path)}\")\n",
    "        return image\n",
    "    \n",
    "    def predict(self, context, model_input, params=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generate captions for input images.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context\n",
    "            model_input: Can be a pandas DataFrame with image column, \n",
    "                        a list of images, or a single image\n",
    "            params: Optional parameters dict with:\n",
    "                   - max_length: Maximum caption length (default: 50)\n",
    "                   - num_beams: Number of beams for beam search (default: 4)\n",
    "                   - text: Optional conditioning text for guided captioning\n",
    "                   \n",
    "        Returns:\n",
    "            pandas DataFrame with 'caption' column or list of captions\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            params = {}\n",
    "            \n",
    "        max_length = params.get('max_length', 50)\n",
    "        num_beams = params.get('num_beams', 4)\n",
    "        conditional_text = params.get('text', None)\n",
    "        model_input = model_input['image_path']\n",
    "        # print(type(model_input))\n",
    "        # print(\"before processing:\", model_input)\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            # Assume first column contains images\n",
    "            # print(\"pandas\")\n",
    "            # print(model_input)\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, pd.Series):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, list):\n",
    "            # print(\"list\")\n",
    "            images = model_input\n",
    "        else:\n",
    "            # Single image\n",
    "            # print(\"single\")\n",
    "            images = [model_input]\n",
    "        \n",
    "        # Load all images\n",
    "        # pil_images = [self._load_image(img) for img in images]\n",
    "        pil_images = [self._convert_image(img) for img in images]\n",
    "        \n",
    "        # Generate captions\n",
    "        captions = []\n",
    "        for pil_image in pil_images:\n",
    "            if conditional_text:\n",
    "                # Conditional captioning with text prompt\n",
    "                inputs = self.processor(\n",
    "                    pil_image, \n",
    "                    conditional_text, \n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "            else:\n",
    "                # Unconditional captioning\n",
    "                inputs = self.processor(\n",
    "                    pil_image, \n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "            \n",
    "            # Generate caption\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams\n",
    "                )\n",
    "            \n",
    "            caption = self.processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "            captions.append(caption)\n",
    "        \n",
    "        # Return as DataFrame\n",
    "        return captions\n",
    "        # return pd.DataFrame({'caption': captions})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01077f5b-a4af-4aad-bd9d-07a36e824ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blip = BLIPCaptioningModel()\n",
    "\n",
    "class ContextObject():\n",
    "  def __init__(self, artifacts):\n",
    "    self.artifacts = artifacts\n",
    "\n",
    "artifacts = {}\n",
    "blip_context = ContextObject(artifacts)\n",
    "\n",
    "blip.load_context(context = blip_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "522d5198-f601-4536-b912-3a2863c18bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_dir = \"/Volumes/pubsec_video_processing/cv/images\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]\n",
    "pil_images = [Image.open(os.path.join(image_dir, f)).convert(\"RGB\") for f in image_files]\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": pil_images + pil_images + pil_images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "479ec971-6c9f-4cf4-a0ef-1c16be5b3ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pil_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d39eb2-7f05-4a12-80a5-98806ce4c404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = blip.predict(\n",
    "  context = None,\n",
    "  model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f1b491-d0eb-4736-9054-ad357225ea63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return base64_string\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/Volumes/pubsec_video_processing/cv/images/bruno.png\"\n",
    "image_path = image_to_base64(image_path)\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": [image_path]\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = blip.predict(\n",
    "  context = None,\n",
    "  model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c7866a-754a-43bf-a14f-2f955f885b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# specify the location the model will be saved/registered in Unity Catalog\n",
    "catalog = \"pubsec_video_processing\"\n",
    "schema = \"cv\"\n",
    "model_name = \"transformers-blip\"\n",
    "model_full_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "# mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "signature = infer_signature(model_input=model_input, model_output=response)\n",
    "\n",
    "# Define conda environment with dependencies\n",
    "conda_env = {\n",
    "    'channels': ['conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.12.3',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'mlflow>=2.10.0',\n",
    "                'torch>=2.0.0',\n",
    "                # 'transformers>=4.30.0',\n",
    "                'git+https://github.com/huggingface/transformers.git'\n",
    "                'Pillow',\n",
    "                'torchvision',\n",
    "                \"cloudpickle==3.0.0\",\n",
    "                # 'pillow>=9.0.0',\n",
    "                'numpy>=1.23.0',\n",
    "                'pandas>=1.5.0',\n",
    "                'accelerate>=0.20.0'\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    'name': 'blip_env'\n",
    "}\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=BLIPCaptioningModel(),\n",
    "        signature=signature,\n",
    "        input_example=model_input,\n",
    "        conda_env=conda_env,\n",
    "        # extra_pip_requirements=[\n",
    "        #   \"torch\",\n",
    "        #   \"git+https://github.com/huggingface/transformers.git\",\n",
    "        #   \"Pillow\"\n",
    "        # ]\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Model registered! URI: runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58152c28-2cbe-438b-a213-5a17f7108d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9f47e87-74e6-48c1-9006-95911048978e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a82ca56e-cb10-4e2a-945b-f8eb0069d61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL.Image.Image\n",
    "        format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 string that can be safely passed in JSON\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=format)\n",
    "    buf.seek(0)\n",
    "    b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return b64_str\n",
    "\n",
    "written_images_base64 = []\n",
    "for pil_img in pil_images + pil_images + pil_images:\n",
    "  written_images_base64.append(pil_to_base64_str(pil_img))\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": written_images_base64\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c83f55-b945-4a02-a952-42bd97034c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the model using the \"run\" from above.\n",
    "mlflow.register_model(model_uri=f\"runs:/{run_id}/model\", name=model_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9c4a75-acd1-4f79-82b2-b812dc96d37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7954e4d1-0b69-4031-984a-dad3b5a534ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6594225-0498-45fb-b69f-d2a9ba6854a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "openai",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model-blip",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
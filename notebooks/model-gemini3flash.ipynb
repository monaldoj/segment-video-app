{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd09af1-5ae4-4e93-aa6c-ca6dbb753329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dd972f1-92ce-4c03-bb05-d81662a6b40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "import io\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "\n",
    "class GeminiFlashModelCaptioning(PythonModel):\n",
    "    \"\"\"\n",
    "    MLflow PyFunc wrapper for Databricks Foundation Model API (e.g., Gemini).\n",
    "    \n",
    "    This model accepts images in multiple formats:\n",
    "    - PIL Image objects\n",
    "    - File paths (strings)\n",
    "    - Base64 encoded strings\n",
    "    - Bytes\n",
    "    - HTTP URLs\n",
    "    \n",
    "    Maintains the same input/output signature as BLIPCaptioningModel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Initialize the Databricks client and endpoint configuration.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context containing artifacts and parameters\n",
    "        \"\"\"\n",
    "        # Initialize Databricks workspace client\n",
    "        self.w = WorkspaceClient()\n",
    "        self.client = self.w.serving_endpoints.get_open_ai_client()\n",
    "        \n",
    "        # Default model endpoint (can be overridden in predict params)\n",
    "        self.default_model_endpoint = \"databricks-gemini-3-flash\"\n",
    "        # self.default_model_endpoint = \"databricks-gemini-2-5-flash\"\n",
    "        \n",
    "    def _convert_image(self, image_path) -> str:\n",
    "        \"\"\"\n",
    "        Convert various image input formats to a format compatible with Qwen3-VL.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Image in various formats (path, bytes, base64, PIL Image, URL)\n",
    "            \n",
    "        Returns:\n",
    "            String representation suitable for Qwen3-VL (file path or URL)\n",
    "        \"\"\"\n",
    "        # print(\"IMAGE PATH:\", image_path)\n",
    "        if isinstance(image_path, Image.Image):\n",
    "            # print(\"IT IS AN IMAGE\")\n",
    "            # Convert PIL Image to base64 data URL\n",
    "            buffered = BytesIO()\n",
    "            image_path.save(buffered, format=\"PNG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        elif isinstance(image_path, str):\n",
    "            # print(\"FIRST LEVEL STRING\")\n",
    "            if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "                # HTTP/HTTPS URL - return as is\n",
    "                # print(\"IT IS AN HTTP\")\n",
    "                return image_path\n",
    "            elif image_path.startswith('/') or image_path.startswith('file://'):\n",
    "                # Local file path\n",
    "                # print(\"IT IS A /\")\n",
    "                if not image_path.startswith('file://'):\n",
    "                    image_path = f\"file://{image_path}\"\n",
    "                return image_path\n",
    "            elif image_path.startswith('data:image'):\n",
    "                # Already a base64 data URL\n",
    "                # print(\"IT IS A base64str\")\n",
    "                return image_path\n",
    "            else:\n",
    "                # Assume it's a base64 string without header\n",
    "                # print(\"IT IS A str to be appended\")\n",
    "                return f\"data:image/png;base64,{image_path}\"\n",
    "        \n",
    "        elif isinstance(image_path, bytes):\n",
    "            # Convert bytes to base64 data URL\n",
    "            # print(\"IT IS bytes\")\n",
    "            img_str = base64.b64encode(image_path).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"❌ Unsupported image input type: {type(image_path)}\")\n",
    "    \n",
    "    def predict(self, context, model_input, params=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generate captions for input images using Databricks Foundation Model API.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context\n",
    "            model_input: Can be a pandas DataFrame with image column, \n",
    "                        a list of images, or a single image\n",
    "            params: Optional parameters dict with:\n",
    "                   - model_endpoint: Model endpoint name (default: databricks-gemini-3-flash)\n",
    "                   - text: Optional conditioning text for guided captioning\n",
    "                   - max_tokens: Maximum tokens to generate\n",
    "                   - temperature: Sampling temperature\n",
    "                   \n",
    "        Returns:\n",
    "            List of caption strings\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            params = {}\n",
    "            \n",
    "        model_endpoint = params.get('model_endpoint', self.default_model_endpoint)\n",
    "        conditional_text = params.get('text', None)\n",
    "        max_tokens = params.get('max_tokens', None)\n",
    "        temperature = params.get('temperature', None)\n",
    "        \n",
    "        # Extract image_path from model_input\n",
    "        model_input = model_input['image_path']\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, pd.Series):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, list):\n",
    "            images = model_input\n",
    "        else:\n",
    "            # Single image\n",
    "            images = [model_input]\n",
    "        \n",
    "        # Load all images\n",
    "        image_urls = [self._convert_image(img) for img in images]\n",
    "        \n",
    "        # Generate captions\n",
    "        captions = []\n",
    "        for image_url in image_urls:\n",
    "            # Create prompt\n",
    "            if conditional_text:\n",
    "                prompt_text = conditional_text\n",
    "            else:\n",
    "                prompt_text = \"Describe this image.\"\n",
    "\n",
    "            # Prepare content for API\n",
    "            content = [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_url}\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Prepare API call parameters\n",
    "            api_params = {\n",
    "                \"model\": model_endpoint,\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }],\n",
    "                \"reasoning_effort\": \"low\"\n",
    "            }\n",
    "            \n",
    "            # Add optional parameters if provided\n",
    "            if max_tokens is not None:\n",
    "                api_params[\"max_tokens\"] = max_tokens\n",
    "            if temperature is not None:\n",
    "                api_params[\"temperature\"] = temperature\n",
    "            \n",
    "            # Call the API\n",
    "            response = self.client.chat.completions.create(**api_params)\n",
    "            \n",
    "            # Extract caption from response\n",
    "            caption = response.choices[0].message.content\n",
    "            captions.append(caption)\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2109a758-b7b9-4b24-8e9c-b14bc631cad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# import cv2\n",
    "\n",
    "# # Extract frames from video stored in UC Volumes\n",
    "# def extract_frames(video_path, num_frames=10):\n",
    "#     \"\"\"Extract N frames from video\"\"\"\n",
    "#     # Implementation using cv2 or similar\n",
    "#     pass\n",
    "\n",
    "# # Process frames using ai_query\n",
    "# result_df = spark.sql(\"\"\"\n",
    "#     SELECT \n",
    "#         video_path,\n",
    "#         ai_query(\n",
    "#             'databricks-gemini-3-flash',\n",
    "#             'Describe what you see in these video frames',\n",
    "#             files => frame_content\n",
    "#         ) as description\n",
    "#     FROM video_frames_table\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d195c2c8-1008-4717-8412-0b92a558bb33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from openai import OpenAI\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import StringType, BinaryType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "class GeminiFlashModelCaptioning(PythonModel):\n",
    "    \"\"\"\n",
    "    MLflow PyFunc wrapper for Databricks Foundation Model API (e.g., Gemini).\n",
    "    \n",
    "    This model accepts images in multiple formats:\n",
    "    - PIL Image objects\n",
    "    - File paths (strings)\n",
    "    - Base64 encoded strings\n",
    "    - Bytes\n",
    "    - HTTP URLs\n",
    "    \n",
    "    Maintains the same input/output signature as BLIPCaptioningModel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Initialize the Databricks client and endpoint configuration.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context containing artifacts and parameters\n",
    "        \"\"\"\n",
    "        # Initialize Databricks workspace client\n",
    "        self.w = WorkspaceClient()\n",
    "        self.client = self.w.serving_endpoints.get_open_ai_client()\n",
    "        \n",
    "        # Default model endpoint (can be overridden in predict params)\n",
    "        self.default_model_endpoint = \"databricks-gemini-3-flash\"\n",
    "        \n",
    "    def _convert_image(self, image_path) -> str:\n",
    "        \"\"\"\n",
    "        Convert various image input formats to a format compatible with Qwen3-VL.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Image in various formats (path, bytes, base64, PIL Image, URL)\n",
    "            \n",
    "        Returns:\n",
    "            String representation suitable for Qwen3-VL (file path or URL)\n",
    "        \"\"\"\n",
    "        # print(\"IMAGE PATH:\", image_path)\n",
    "        if isinstance(image_path, Image.Image):\n",
    "            # print(\"IT IS AN IMAGE\")\n",
    "            # Convert PIL Image to base64 data URL\n",
    "            buffered = BytesIO()\n",
    "            image_path.save(buffered, format=\"PNG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        elif isinstance(image_path, str):\n",
    "            # print(\"FIRST LEVEL STRING\")\n",
    "            if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
    "                # HTTP/HTTPS URL - return as is\n",
    "                # print(\"IT IS AN HTTP\")\n",
    "                return image_path\n",
    "            elif image_path.startswith('/') or image_path.startswith('file://'):\n",
    "                # Local file path\n",
    "                # print(\"IT IS A /\")\n",
    "                if not image_path.startswith('file://'):\n",
    "                    image_path = f\"file://{image_path}\"\n",
    "                return image_path\n",
    "            elif image_path.startswith('data:image'):\n",
    "                # Already a base64 data URL\n",
    "                # print(\"IT IS A base64str\")\n",
    "                return image_path\n",
    "            else:\n",
    "                # Assume it's a base64 string without header\n",
    "                # print(\"IT IS A str to be appended\")\n",
    "                return f\"data:image/png;base64,{image_path}\"\n",
    "        \n",
    "        elif isinstance(image_path, bytes):\n",
    "            # Convert bytes to base64 data URL\n",
    "            # print(\"IT IS bytes\")\n",
    "            img_str = base64.b64encode(image_path).decode()\n",
    "            return f\"data:image/png;base64,{img_str}\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"❌ Unsupported image input type: {type(image_path)}\")\n",
    "    \n",
    "    def predict(self, context, model_input, params=None) -> list[str]:\n",
    "        \"\"\"\n",
    "        Generate captions for input images using Databricks Foundation Model API.\n",
    "        \n",
    "        Args:\n",
    "            context: MLflow context\n",
    "            model_input: Can be a pandas DataFrame with image column, \n",
    "                        a list of images, or a single image\n",
    "            params: Optional parameters dict with:\n",
    "                   - model_endpoint: Model endpoint name (default: databricks-gemini-3-flash)\n",
    "                   - text: Optional conditioning text for guided captioning\n",
    "                   - max_tokens: Maximum tokens to generate\n",
    "                   - temperature: Sampling temperature\n",
    "                   \n",
    "        Returns:\n",
    "            List of caption strings\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            params = {}\n",
    "            \n",
    "        model_endpoint = params.get('model_endpoint', self.default_model_endpoint)\n",
    "        conditional_text = params.get('text', None)\n",
    "        max_tokens = params.get('max_tokens', None)\n",
    "        temperature = params.get('temperature', None)\n",
    "        \n",
    "        # Extract image_path from model_input\n",
    "        model_input = model_input['image_path']\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, pd.Series):\n",
    "            images = model_input.tolist()[0]\n",
    "        elif isinstance(model_input, list):\n",
    "            images = model_input\n",
    "        else:\n",
    "            # Single image\n",
    "            images = [model_input]\n",
    "        \n",
    "        # Load all images\n",
    "        image_urls = [self._convert_image(img) for img in images]\n",
    "        prompt_text = conditional_text if conditional_text else \"Describe this image.\"\n",
    "        prompts = []\n",
    "        msgs = []\n",
    "\n",
    "        for image_url in image_urls:\n",
    "            # Create prompt\n",
    "            if conditional_text:\n",
    "                prompt_text = conditional_text\n",
    "            else:\n",
    "                prompt_text = \"Describe this image.\"\n",
    "\n",
    "            # Prepare content for API\n",
    "            content = [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_url}\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Prepare API call parameters\n",
    "            api_params = {\n",
    "                \"model\": model_endpoint,\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content\n",
    "                }],\n",
    "                \"reasoning_effort\": \"low\"\n",
    "            }\n",
    "\n",
    "            # Build named_struct for ai_query from api_params['messages'][0]\n",
    "            msg = api_params['messages'][0]\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "\n",
    "            content_expr = (\n",
    "                f\"array(\"\n",
    "                f\"named_struct('type', 'text', 'text', '{prompt_text}'),\"\n",
    "                f\"named_struct('type', 'image_url', 'image_url', named_struct('url', '{image_url}'))\"\n",
    "                f\")\"\n",
    "            )\n",
    "\n",
    "            named_struct_expr = f\"named_struct('role', '{role}', 'content', {content_expr})\"\n",
    "            msgs.append(named_struct_expr)\n",
    "\n",
    "        print(msgs)\n",
    "        df = spark.createDataFrame([(msg,) for msg in msgs], [\"msg\"])\n",
    "        print(df)\n",
    "        query = f\"\"\"\n",
    "                        ai_query(\n",
    "                            \"{model_endpoint}\",\n",
    "                            request => named_struct(\"messages\",\n",
    "                                ARRAY({msgs[0]}))\n",
    "                        )\n",
    "            \"\"\"\n",
    "        print(query)\n",
    "        # return(query)\n",
    "        df_with_captions = df.withColumn(\n",
    "            \"caption\",\n",
    "            F.expr(query),    \n",
    "        )\n",
    "\n",
    "        # final_df = df_with_captions.withColumn(\n",
    "        #     \"caption\", \n",
    "        #     F.col(\"response.choices\")[0][\"message\"][\"content\"]\n",
    "        # )\n",
    "\n",
    "        # Collect captions as a list\n",
    "        captions = [row.caption for row in df_with_captions.select(\"caption\").collect()]\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d215e653-54a1-4efe-8a2b-3ed9e15b3584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flash = GeminiFlashModelCaptioning()\n",
    "\n",
    "class ContextObject():\n",
    "  def __init__(self, artifacts):\n",
    "    self.artifacts = artifacts\n",
    "\n",
    "artifacts = {}\n",
    "flash_context = ContextObject(artifacts)\n",
    "\n",
    "flash.load_context(context = flash_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdcd4f75-51f3-400b-9e78-0dad84133a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_dir = \"/Volumes/pubsec_video_processing/cv/images\"\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.png')]\n",
    "pil_images = [Image.open(os.path.join(image_dir, f)).convert(\"RGB\") for f in image_files]\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": pil_images + pil_images + pil_images\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a19aab8d-b39c-446a-813b-af366d94713a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pil_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860cc7c5-0405-49ce-84c0-760433da73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = flash.predict(\n",
    "  context = None,\n",
    "  model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04fd6b5c-5d89-4769-928a-2963c1f3348e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import base64\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return base64_string\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/Volumes/pubsec_video_processing/cv/images/bruno.png\"\n",
    "image_path = image_to_base64(image_path)\n",
    "print(type(image_path))\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": [image_path]*5\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "response = flash.predict(\n",
    "  context = None,\n",
    "  model_input = model_input\n",
    ")\n",
    "# print(response.iloc[0].caption)\n",
    "print(response)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9556c57e-781c-407a-8433-5585e3127710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# specify the location the model will be saved/registered in Unity Catalog\n",
    "catalog = \"pubsec_video_processing\"\n",
    "schema = \"cv\"\n",
    "model_name = \"transformers-qwen3-2B-vision\"\n",
    "model_full_name = f\"{catalog}.{schema}.{model_name}\"\n",
    "# mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "signature = infer_signature(model_input=model_input, model_output=response)\n",
    "\n",
    "# Define conda environment with dependencies\n",
    "conda_env = {\n",
    "    'channels': ['conda-forge', 'defaults'],\n",
    "    'dependencies': [\n",
    "        'python=3.12.3',\n",
    "        'pip',\n",
    "        {\n",
    "            'pip': [\n",
    "                'mlflow>=2.10.0',\n",
    "                'torch>=2.0.0',\n",
    "                # 'transformers>=4.30.0',\n",
    "                'git+https://github.com/huggingface/transformers.git'\n",
    "                'Pillow',\n",
    "                'torchvision',\n",
    "                \"cloudpickle==3.0.0\",\n",
    "                # 'pillow>=9.0.0',\n",
    "                'numpy>=1.23.0',\n",
    "                'pandas>=1.5.0',\n",
    "                'accelerate>=0.20.0'\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    'name': 'blip_env'\n",
    "}\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=Qwen3CaptioningModel(),\n",
    "        signature=signature,\n",
    "        input_example=model_input,\n",
    "        conda_env=conda_env,\n",
    "        # extra_pip_requirements=[\n",
    "        #   \"torch\",\n",
    "        #   \"git+https://github.com/huggingface/transformers.git\",\n",
    "        #   \"Pillow\"\n",
    "        # ]\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Model registered! URI: runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f56bedf-950b-4f8b-9ea6-c1898a0410c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef354e7f-c42a-4b53-911b-44916a7f8172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63804fd2-449c-42a0-be04-9f47e22a5916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "    Args:\n",
    "        img: PIL.Image.Image\n",
    "        format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 string that can be safely passed in JSON\n",
    "    \"\"\"\n",
    "    buf = BytesIO()\n",
    "    img.save(buf, format=format)\n",
    "    buf.seek(0)\n",
    "    b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "    return b64_str\n",
    "\n",
    "written_images_base64 = []\n",
    "for pil_img in pil_images + pil_images + pil_images:\n",
    "  written_images_base64.append(pil_to_base64_str(pil_img))\n",
    "\n",
    "model_input = {\n",
    "  \"image_path\": written_images_base64\n",
    "}\n",
    "\n",
    "starting_time = timeit.default_timer()\n",
    "model_output = loaded_model.predict(model_input)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time)*1000)}ms\")\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13692a64-d02f-4f8b-a5d1-e1aa62ba1d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register the model using the \"run\" from above.\n",
    "mlflow.register_model(model_uri=f\"runs:/{run_id}/model\", name=model_full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e06f4d-2885-41d2-a183-6d4f2a0b5999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baff0f5b-5b54-4513-8dc0-d4b86632e406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad889946-3ee5-47e8-b373-3f51990da631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "openai",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model-gemini3flash",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e73c4d3a-6cb0-4696-8059-9d5203d1d9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trigger_location = dbutils.widgets.get(\"trigger_location\")\n",
    "prompt = dbutils.widgets.get(\"prompt\")\n",
    "frame_stride = int(dbutils.widgets.get(\"frame_stride\"))\n",
    "truncate = dbutils.widgets.get(\"truncate\")\n",
    "threshold = float(dbutils.widgets.get(\"threshold\"))\n",
    "\n",
    "# trigger_location = '/Volumes/pubsec_video_processing/cv/auto_segment/images/maren_jack.MOV'\n",
    "# prompt = 'girl in pink sweatshrit'\n",
    "# frame_stride = 30\n",
    "# truncate = True\n",
    "# threshold = 0.2\n",
    "\n",
    "if truncate==0 or truncate=='false' or truncate=='False':\n",
    "    truncate = False\n",
    "else:\n",
    "    truncate=True\n",
    "\n",
    "print(trigger_location)\n",
    "print(prompt)\n",
    "print(frame_stride)\n",
    "print(truncate)\n",
    "print(type(truncate))\n",
    "print(threshold)\n",
    "print(type(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84498fbc-1269-47ff-a7fb-cdab90ff2136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.notebook.exit(\"Notebook execution stopped by user request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "479ea14e-ac82-43ec-b6bc-db578adc9f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import base64\n",
    "import timeit\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8123854-9005-46c4-83fb-099ea44cfb4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if trigger_location.endswith('/') and (trigger_location[-4]!='.' or trigger_location[-5]!='.'):\n",
    "    # Your volume directory path\n",
    "    directory_path = trigger_location\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "    # Filter out directories, keep only files\n",
    "    files = [f for f in files if not f.isDir()]\n",
    "\n",
    "    # Sort by modification time (most recent first)\n",
    "    files_sorted = sorted(files, key=lambda x: x.modificationTime, reverse=True)\n",
    "\n",
    "    # Get the most recent file\n",
    "    if files_sorted:\n",
    "            most_recent_file = files_sorted[0]\n",
    "            most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "            most_recent_name = most_recent_file.name\n",
    "            \n",
    "            print(f\"Most recent file: {most_recent_name}\")\n",
    "            print(f\"Full path: {most_recent_path}\")\n",
    "            print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")\n",
    "    else:\n",
    "            print(\"No files found in directory\")\n",
    "else:\n",
    "    most_recent_file = dbutils.fs.ls(trigger_location)[0]\n",
    "    most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "    most_recent_name = most_recent_file.name\n",
    "\n",
    "    print(f\"Most recent file: {most_recent_name}\")\n",
    "    print(f\"Full path: {most_recent_path}\")\n",
    "    print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6a1b12-e3d2-4218-b1e4-d8b16655c072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most_recent_file = trigger_location\n",
    "# most_recent_name = most_recent_file.split(\"/\")[-1]\n",
    "# most_recent_path = most_recent_file.replace('dbfs:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a7503d2-ef89-4016-a7da-bf7258b2478b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_pat = dbutils.secrets.get(\"justinm-buildathon-secrets\", \"hf_pat\")\n",
    "os.environ[\"HF_TOKEN\"] = hf_pat\n",
    "login(token=hf_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06278b6e-766d-4700-b79f-67d2493e791b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-sam3-video@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daff359f-817c-48a3-aabd-10857551e457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_results(FILE_URL, results):\n",
    "    import os\n",
    "\n",
    "    if FILE_URL.startswith(\"/Volumes/pubsec_video_processing/cv/auto_segment/inputs/\"):\n",
    "    OUTPUT_FILE_URL = FILE_URL.replace(\"inputs\", \"outputs\")\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "    else:\n",
    "    OUTPUT_FILE_URL = most_recent_path.replace(most_recent_name, f\"outputs/{most_recent_name}\")\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    FPS = 24\n",
    "\n",
    "    def decode_mask(encoded_mask: str) -> np.ndarray:\n",
    "            \"\"\"Decode base64 mask back to numpy array\"\"\"\n",
    "            buf = BytesIO(base64.b64decode(encoded_mask))\n",
    "            return np.load(buf)\n",
    "\n",
    "    def add_timestamp(frame: np.ndarray, timestamp_sec: float) -> np.ndarray:\n",
    "            \"\"\"Add timestamp overlay to frame in the top-right corner\"\"\"\n",
    "            # Convert seconds to HH:MM:SS.ms format\n",
    "            hours = int(timestamp_sec // 3600)\n",
    "            minutes = int((timestamp_sec % 3600) // 60)\n",
    "            seconds = int(timestamp_sec % 60)\n",
    "            milliseconds = int((timestamp_sec % 1) * 1000)\n",
    "            \n",
    "            timestamp_text = f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "            \n",
    "            # Create a copy to avoid modifying original\n",
    "            frame_with_timestamp = frame.copy()\n",
    "            \n",
    "            # Get frame dimensions\n",
    "            height, width = frame.shape[:2]\n",
    "            \n",
    "            # Set up text properties\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.7\n",
    "            font_thickness = 2\n",
    "            text_color = (255, 255, 255)  # White text\n",
    "            bg_color = (0, 0, 0)  # Black background\n",
    "            padding = 10\n",
    "            \n",
    "            # Get text size\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                    timestamp_text, font, font_scale, font_thickness\n",
    "            )\n",
    "            \n",
    "            # Position in top-right corner\n",
    "            text_x = width - text_width - padding\n",
    "            text_y = padding + text_height\n",
    "            \n",
    "            # Draw semi-transparent background rectangle\n",
    "            bg_x1 = text_x - 5\n",
    "            bg_y1 = text_y - text_height - 5\n",
    "            bg_x2 = text_x + text_width + 5\n",
    "            bg_y2 = text_y + baseline + 5\n",
    "            \n",
    "            # Create overlay for semi-transparency\n",
    "            overlay = frame_with_timestamp.copy()\n",
    "            cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), bg_color, -1)\n",
    "            cv2.addWeighted(overlay, 0.6, frame_with_timestamp, 0.4, 0, frame_with_timestamp)\n",
    "            \n",
    "            # Draw text\n",
    "            cv2.putText(\n",
    "                    frame_with_timestamp,\n",
    "                    timestamp_text,\n",
    "                    (text_x, text_y),\n",
    "                    font,\n",
    "                    font_scale,\n",
    "                    text_color,\n",
    "                    font_thickness,\n",
    "                    cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "            return frame_with_timestamp\n",
    "\n",
    "    # Open original video to get frames\n",
    "    print(\"Processing frames and applying masks...\")\n",
    "    cap = cv2.VideoCapture(FILE_URL)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or FPS\n",
    "\n",
    "    # Create a mapping of frame_idx to results\n",
    "    print(f\"Found {len(results)} frames in results\")\n",
    "    result_map = {r[\"frame_idx\"]: r for r in results}\n",
    "\n",
    "    frame_idx = 0\n",
    "    saved_images = []\n",
    "    segmented_images = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                    break\n",
    "            \n",
    "            # Only process frames with segmentation results\n",
    "            if frame_idx in result_map:\n",
    "                    # Convert BGR to RGB\n",
    "                    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    res = result_map[frame_idx]\n",
    "                    \n",
    "                    # if res[\"masks\"]:\n",
    "                    #     # Get the first (highest score) mask\n",
    "                    #     mask = decode_mask(res[\"masks\"][0])\n",
    "                            \n",
    "                    #     # Overlay with transparency\n",
    "                    #     overlay = rgb_frame.copy()\n",
    "                    #     overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "                    #     masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "\n",
    "                    if res[\"masks\"]:\n",
    "                            # Get the first (highest score) mask\n",
    "                            masks = [decode_mask(x) for x in res[\"masks\"]]\n",
    "                            \n",
    "                            # Overlay with transparency\n",
    "                            overlay = rgb_frame.copy()\n",
    "                            for mask in masks:\n",
    "                                    overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "                            masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "                            \n",
    "                    else:\n",
    "                            masked_frame = rgb_frame\n",
    "                    \n",
    "                    # Calculate timestamp for this frame\n",
    "                    timestamp_sec = frame_idx / fps\n",
    "                    \n",
    "                    # Add timestamp overlay (convert back to BGR for cv2 operations, then back to RGB)\n",
    "                    masked_frame_bgr = cv2.cvtColor(masked_frame, cv2.COLOR_RGB2BGR)\n",
    "                    masked_frame_with_timestamp = add_timestamp(masked_frame_bgr, timestamp_sec)\n",
    "                    masked_frame = cv2.cvtColor(masked_frame_with_timestamp, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Save frame\n",
    "                    if not truncate:\n",
    "                            saved_images.append(Image.fromarray(rgb_frame))\n",
    "                            segmented_images.append(Image.fromarray(masked_frame))\n",
    "                    elif res[\"masks\"]:\n",
    "                            saved_images.append(Image.fromarray(rgb_frame))\n",
    "                            segmented_images.append(Image.fromarray(masked_frame))\n",
    "            \n",
    "            frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Saved {len(saved_images)} frames to memory\")\n",
    "    print(f\"Saved {len(segmented_images)} segmented frames to memory\")\n",
    "\n",
    "    # 3. Create full segmented video\n",
    "    import imageio\n",
    "    import os\n",
    "    import shutil\n",
    "    import tempfile\n",
    "\n",
    "    print(\"Writing video to temporary file...\")\n",
    "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:\n",
    "            temp_video_path = tmp_file.name\n",
    "\n",
    "    imageio.mimsave(\n",
    "            temp_video_path,\n",
    "            segmented_images,\n",
    "            fps=(fps/frame_stride) * 5,  # Use original FPS instead of hardcoded 24\n",
    "            codec='libx264',\n",
    "            pixelformat='yuv420p'\n",
    "    )\n",
    "\n",
    "    temp_size = os.path.getsize(temp_video_path)\n",
    "    print(f\"Temporary video created: {temp_size:,} bytes ({temp_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "    # Copy to Volumes\n",
    "    print(f\"Copying to Volumes: {OUTPUT_FILE_URL}\")\n",
    "    shutil.copy2(temp_video_path, OUTPUT_FILE_URL)\n",
    "\n",
    "    final_size = os.path.getsize(OUTPUT_FILE_URL)\n",
    "    print(f\"âœ“ Video successfully saved to: {OUTPUT_FILE_URL}\")\n",
    "    print(f\"  Final size: {final_size:,} bytes ({final_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "    # Clean up temporary file\n",
    "    if os.path.exists(temp_video_path):\n",
    "            os.remove(temp_video_path)\n",
    "            print(\"Cleaned up temporary file\")\n",
    "\n",
    "    return saved_images\n",
    "#   return OUTPUT_FILE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3130f986-618b-4e30-a1e8-ec697cc1fbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_file(triggered_file, prompt):\n",
    "    print(f\"Triggered by file: {triggered_file}\")\n",
    "    # Your processing logic here\n",
    "    model_input = pd.DataFrame([{\n",
    "        \"video_path\": triggered_file,\n",
    "        \"prompt\": prompt,\n",
    "        \"frame_stride\": frame_stride,  # Process every nth frame\n",
    "        \"batch_size\": 4,\n",
    "        \"threshold\": threshold,\n",
    "        \"mask_threshold\": 0.5\n",
    "    }])\n",
    "    results = model.predict(model_input)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eb2b31a-52ce-43cb-8091-3fca77983633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Files processed in this run:\", most_recent_path)\n",
    "\n",
    "print(\"Segmenting video...\")\n",
    "starting_time = timeit.default_timer()\n",
    "results = process_file(most_recent_path, prompt)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time))} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73deb09-7aab-4b23-92ff-b057495c284b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# output_file_url = write_results(most_recent_path, results)\n",
    "saved_images = write_results(most_recent_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71ee8fb9-93f6-4322-9daa-7fd8fcfc42df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy\n",
    "import base64\n",
    "import cv2\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI, ChatCompletion\n",
    "from typing import NamedTuple, List, Optional, Generator, Tuple\n",
    "\n",
    "# helpers\n",
    "class FrameBatch(NamedTuple):\n",
    "    content: List[dict]\n",
    "    sizes: List[int]\n",
    "    total_bytes: int\n",
    "    \n",
    "    @property\n",
    "    def frame_count(self) -> int:\n",
    "        return len(self.content)\n",
    "\n",
    "def encode_jpeg(frame: np.ndarray, quality: int) -> bytes:\n",
    "    \"\"\"cv2.imencode is 3-5x faster than PIL, no color conversion needed.\"\"\"\n",
    "    _, buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    return buf.tobytes()\n",
    "\n",
    "def to_content(jpg_bytes: bytes) -> dict:\n",
    "    b64 = base64.b64encode(jpg_bytes).decode('ascii')\n",
    "    return {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{b64}'}}\n",
    "\n",
    "def fit_to_size(frame: np.ndarray, max_size: int) -> Tuple[bytes, int]:\n",
    "    \n",
    "    # Early exit - most frames fit at q=95\n",
    "    data = encode_jpeg(frame, 95)\n",
    "    if len(data) <= max_size:\n",
    "        return data, 95\n",
    "    \n",
    "    # binary search FTW!\n",
    "    lo, hi, best = 10, 94, (encode_jpeg(frame, 10), 10)\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) >> 1\n",
    "        data = encode_jpeg(frame, mid)\n",
    "        if len(data) <= max_size:\n",
    "            best, lo = (data, mid), mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "    return best\n",
    "\n",
    "def process_frame(\n",
    "    frame: np.ndarray, \n",
    "    quality: Optional[int] = None,\n",
    "    max_size: int = 500_000\n",
    ") -> Tuple[dict, int, int]:\n",
    "    if quality:\n",
    "        data = encode_jpeg(frame, quality)\n",
    "    else:\n",
    "        data, quality = fit_to_size(frame, max_size)\n",
    "    return to_content(data), quality, len(data)\n",
    "\n",
    "def stream_content(\n",
    "    video: cv2.VideoCapture,\n",
    "    quality: Optional[int] = None,\n",
    "    max_size: int = 500_000\n",
    ") -> Generator[Tuple[dict, int, int], None, None]:\n",
    "    while True:\n",
    "        ok, frame = video.read()\n",
    "        if not ok:\n",
    "            return\n",
    "        yield process_frame(frame, quality, max_size)\n",
    "\n",
    "def batch_content(\n",
    "    video: cv2.VideoCapture,\n",
    "    quality: Optional[int] = None,\n",
    "    max_frame_size: int = 500_000,\n",
    "    max_batch_size: int = 3_000_000\n",
    ") -> Generator[FrameBatch, None, None]:\n",
    "    max_frame_size = min(max_frame_size, max_batch_size)\n",
    "    batch, sizes, batch_size = [], [], 0\n",
    "    for content, _, size in stream_content(video, quality, max_frame_size):\n",
    "        if batch and batch_size + size > max_batch_size:\n",
    "            yield FrameBatch(batch, sizes, batch_size)\n",
    "            batch, sizes, batch_size = [], [], 0\n",
    "        batch.append(content)\n",
    "        sizes.append(size)\n",
    "        batch_size += size\n",
    "    \n",
    "    if batch:\n",
    "        yield FrameBatch(batch, sizes, batch_size)\n",
    "\n",
    "def stream_content_from_images(\n",
    "    images: List[Image.Image],\n",
    "    quality: Optional[int] = None,\n",
    "    max_size: int = 500_000\n",
    ") -> Generator[Tuple[dict, int, int], None, None]:\n",
    "    for img in images:\n",
    "        # Convert PIL Image to numpy array (OpenCV format)\n",
    "        frame = np.array(img)\n",
    "        # Convert RGB to BGR if needed (OpenCV uses BGR)\n",
    "        if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        yield process_frame(frame, quality, max_size)\n",
    "\n",
    "def batch_content_from_images(\n",
    "    images: List[Image.Image],\n",
    "    quality: Optional[int] = None,\n",
    "    max_frame_size: int = 500_000,\n",
    "    max_batch_size: int = 3_000_000\n",
    ") -> Generator[FrameBatch, None, None]:\n",
    "    max_frame_size = min(max_frame_size, max_batch_size)\n",
    "    batch, sizes, batch_size = [], [], 0\n",
    "    for content, _, size in stream_content_from_images(images, quality, max_frame_size):\n",
    "        if batch and batch_size + size > max_batch_size:\n",
    "            yield FrameBatch(batch, sizes, batch_size)\n",
    "            batch, sizes, batch_size = [], [], 0\n",
    "        batch.append(content)\n",
    "        sizes.append(size)\n",
    "        batch_size += size\n",
    "    \n",
    "    if batch:\n",
    "        yield FrameBatch(batch, sizes, batch_size)\n",
    "\n",
    "def summarize_frames(frame_batch: FrameBatch):\n",
    "    return oai.chat.completions.create(\n",
    "    model=FMAPI_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': [\n",
    "            {'type': 'text', 'text': \n",
    "                'Describe what is happening in the following sequence of images '\n",
    "                'which represent a frames from a video. Describe what is going on in from the cameras '\n",
    "                'perspective. Be descriptive - but succinct and functional'\n",
    "            },\n",
    "            *frame_batch.content\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "    \n",
    "def get_content(completion: ChatCompletion) -> str:\n",
    "    '''\n",
    "        get_content extracts the textual content.  Required with gemini 3 models\n",
    "        as g3 model's content may (or may not) include a thoughtSignature which \n",
    "        is a reference to the model's reasoning process.\n",
    "    '''\n",
    "    content = completion.choices[0].message.content\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    if isinstance(content, list):\n",
    "        return content[0]['text']\n",
    "    else:\n",
    "        print(type(content))\n",
    "        print(content)\n",
    "        raise Exception('weird content signature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa51943-3e6f-4df6-bad5-1641a5e8d734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# actual running code\n",
    "# vid_path = output_file_url\n",
    "# video = cv2.VideoCapture(vid_path)\n",
    "# frame_batches = list(batch_content(video, quality=80))\n",
    "# video.release()\n",
    "\n",
    "frame_batches = list(batch_content_from_images(saved_images, quality=80))\n",
    "\n",
    "FMAPI_SERVING_URL = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}'\n",
    "FMAPI_BASE_URL = f'{FMAPI_SERVING_URL}/serving-endpoints'\n",
    "print(f'ðŸŽ¯ FMAPI_BASE_URL: {FMAPI_BASE_URL}')\n",
    "FMAPI_API_TOKEN = dbutils.secrets.get('justinm-buildathon-secrets', 'db_pat')\n",
    "# Serving endpoint I created using my own GCP acc't\n",
    "# to side-step issues w/ limits databricks hosted model issues.\n",
    "# FMAPI_MODEL = 'stsu-gemini-3-flash' \n",
    "FMAPI_MODEL = 'databricks-gemini-2-5-flash'\n",
    "\n",
    "# clients\n",
    "oai = AsyncOpenAI(\n",
    "    api_key = FMAPI_API_TOKEN,\n",
    "    base_url = FMAPI_BASE_URL\n",
    ")\n",
    "\n",
    "# Get results\n",
    "ai_results = await asyncio.gather(*[\n",
    "    summarize_frames(frame_batch)\n",
    "    for frame_batch in frame_batches\n",
    "])\n",
    "\n",
    "# concatenation of previous generations\n",
    "context = []\n",
    "for idx, result in enumerate(ai_results):\n",
    "    description = f'SECTION {idx}\\n' + get_content(result)\n",
    "    context.append(description)\n",
    "\n",
    "context = '\\n'.join(context)\n",
    "\n",
    "video_summary = await oai.chat.completions.create(\n",
    "    model=FMAPI_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': [\n",
    "            {'type': 'text', 'text': \n",
    "                'Summarize the following sections of a video with a focus on '\n",
    "                'what is functionally happening over time. Be descriptive. '\n",
    "                'The resulting summary should be a functional narrative. '\n",
    "                'Return this as Markdown text - and only markdown. '\n",
    "            },\n",
    "            {'type': 'text', 'text': context}\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "text = get_content(video_summary)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ce5adb-9cb7-40dc-a9c2-9de67113a56a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "print(txt_filename)\n",
    "\n",
    "dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77eb96e-99e2-4eb4-b9d8-b00a1a6a776c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4874a854-6fe6-4d3d-847c-e6bd3ccb37a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e3e2a9d-cc92-4aba-ad60-2608a60e04ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# captioning_model = mlflow.pyfunc.load_model(\"models:/pubsec_video_processing.cv.transformers-blip@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18afe21e-14d8-4fff-861f-7f871e0436a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# import base64\n",
    "\n",
    "# def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "#     \"\"\"\n",
    "#     Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "#     Args:\n",
    "#         img: PIL.Image.Image\n",
    "#         format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Base64 string that can be safely passed in JSON\n",
    "#     \"\"\"\n",
    "#     buf = BytesIO()\n",
    "#     img.save(buf, format=format)\n",
    "#     buf.seek(0)\n",
    "#     b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "#     return b64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d81187c-8a2e-46fd-8bb5-00c6eded8af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from io import BytesIO\n",
    "\n",
    "# written_images_str = [pil_to_base64_str(x) for x in written_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47fb990-ad9a-42aa-bd59-9317d22dd049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_input = {\n",
    "#     \"image_path\": written_images_str\n",
    "# }\n",
    "\n",
    "# captions = captioning_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4252ecd5-87f8-48ed-b943-af8a62a5b71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "# from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "\n",
    "# # Initialize the client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Define your endpoint name\n",
    "# endpoint_name = \"databricks-gpt-oss-120b\"\n",
    "\n",
    "# # # Create a chat message\n",
    "# # messages = [\n",
    "# #     ChatMessage(\n",
    "# #         role=ChatMessageRole.USER,\n",
    "# #         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. Can you identify anything out of the ordinary or anomalous in the video? When I say anomlous, I mean actions taking place that are against the law or violent in nature. If you find something that shouldn't be happening, describe why it is anomolous. If you don't find anything abnormal, just respond with 'Nothing anomolous found.' \\n\\n {captions}\"\n",
    "# #     )\n",
    "# # ]\n",
    "# # Create a chat message\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. There may be obvious scene changes based on the descriptions. Please summarize the contents of each scene in the video in a single sentence each.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# # Query the endpoint\n",
    "# response = w.serving_endpoints.query(\n",
    "#     name=endpoint_name,\n",
    "#     messages=messages,\n",
    "#     # max_tokens=500  # optional parameter\n",
    "# )\n",
    "\n",
    "# # Access the response\n",
    "# r = response.choices[0].message.content\n",
    "# # print(r)\n",
    "# text = [x for x in r if x['type'] == 'text'][0]['text']\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ddc3a40-816f-4580-b082-5ffdf1efcd2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "# txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "# print(txt_filename)\n",
    "\n",
    "# dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28bb0c93-472a-4637-966f-955e3333075b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio[ffmpeg,pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "openai",
     "nvidia-ml-py"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "job-auto-segment-video-fmapi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

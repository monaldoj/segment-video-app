{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9932e4-3836-4923-93b6-ea4a5a5ea7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/justinm/cv/auto_segment/inputs/usps_youtube_example.mov\nemployees or postal workers handling or delivering mail\n30\nTrue\n<class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "trigger_location = dbutils.widgets.get(\"trigger_location\")\n",
    "prompt = dbutils.widgets.get(\"prompt\")\n",
    "frame_stride = int(dbutils.widgets.get(\"frame_stride\"))\n",
    "truncate = dbutils.widgets.get(\"truncate\")\n",
    "\n",
    "# trigger_location = '/Volumes/justinm/cv/auto_segment/inputs/usps_youtube_example.mov'\n",
    "# prompt = 'employees or postal workers handling or delivering mail'\n",
    "# frame_stride = 30\n",
    "# truncate = True\n",
    "\n",
    "if truncate==0 or truncate=='false' or truncate=='False':\n",
    "  truncate = False\n",
    "else:\n",
    "  truncate=True\n",
    "\n",
    "print(trigger_location)\n",
    "print(prompt)\n",
    "print(frame_stride)\n",
    "print(truncate)\n",
    "print(type(truncate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96eab01a-766e-425a-b371-ddea9aeaad28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.notebook.exit(\"Notebook execution stopped by user request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2421fc66-d747-4c14-8f7d-2b659b98cbd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import base64\n",
    "import timeit\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4c2931-c8aa-4afc-9c9b-92000c269c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent file: usps_youtube_example.mov\nFull path: /Volumes/justinm/cv/auto_segment/inputs/usps_youtube_example.mov\nModified: 2026-01-02 23:22:36\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if trigger_location.endswith('/') and (trigger_location[-4]!='.' or trigger_location[-5]!='.'):\n",
    "  # Your volume directory path\n",
    "  directory_path = trigger_location\n",
    "\n",
    "  # List all files in the directory\n",
    "  files = dbutils.fs.ls(directory_path)\n",
    "\n",
    "  # Filter out directories, keep only files\n",
    "  files = [f for f in files if not f.isDir()]\n",
    "\n",
    "  # Sort by modification time (most recent first)\n",
    "  files_sorted = sorted(files, key=lambda x: x.modificationTime, reverse=True)\n",
    "\n",
    "  # Get the most recent file\n",
    "  if files_sorted:\n",
    "      most_recent_file = files_sorted[0]\n",
    "      most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "      most_recent_name = most_recent_file.name\n",
    "      \n",
    "      print(f\"Most recent file: {most_recent_name}\")\n",
    "      print(f\"Full path: {most_recent_path}\")\n",
    "      print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")\n",
    "  else:\n",
    "      print(\"No files found in directory\")\n",
    "else:\n",
    "  most_recent_file = dbutils.fs.ls(trigger_location)[0]\n",
    "  most_recent_path = most_recent_file.path.replace('dbfs:','')\n",
    "  most_recent_name = most_recent_file.name\n",
    "\n",
    "  print(f\"Most recent file: {most_recent_name}\")\n",
    "  print(f\"Full path: {most_recent_path}\")\n",
    "  print(f\"Modified: {datetime.fromtimestamp(most_recent_file.modificationTime/1000)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da64b5b-c1db-4192-adfd-7bb299d80533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# most_recent_file = trigger_location\n",
    "# most_recent_name = most_recent_file.split(\"/\")[-1]\n",
    "# most_recent_path = most_recent_file.replace('dbfs:','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1e8fd6-64d4-470f-8769-c1aff52982d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_pat = dbutils.secrets.get(\"justin-fe-secrets\", \"hf_pat\")\n",
    "os.environ[\"HF_TOKEN\"] = hf_pat\n",
    "login(token=hf_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d976d93b-5b4f-4bc5-b96f-cfe659fd6bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbb1defcc6e4a3393d6b26fc6c2cdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d6f36419484c55a8799bd6eb8cbb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /local_disk0/user_tmp_data/spark-31c609ec-9e21-494a-8e46-c0/tmp82su_9sj/python_model.pkl:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/justinm.cv.transformers-sam3-video@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbde7081-11ad-4b1c-b192-92766db808ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_results(FILE_URL, results):\n",
    "  import os\n",
    "\n",
    "  if FILE_URL.startswith(\"/Volumes/justinm/cv/auto_segment/inputs/\"):\n",
    "    OUTPUT_FILE_URL = FILE_URL.replace(\"inputs\", \"outputs\")\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "  else:\n",
    "    OUTPUT_FILE_URL = most_recent_path.replace(most_recent_name, f\"outputs/{most_recent_name}\")\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_URL)\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  FPS = 24\n",
    "\n",
    "  def decode_mask(encoded_mask: str) -> np.ndarray:\n",
    "      \"\"\"Decode base64 mask back to numpy array\"\"\"\n",
    "      buf = BytesIO(base64.b64decode(encoded_mask))\n",
    "      return np.load(buf)\n",
    "\n",
    "  def add_timestamp(frame: np.ndarray, timestamp_sec: float) -> np.ndarray:\n",
    "      \"\"\"Add timestamp overlay to frame in the top-right corner\"\"\"\n",
    "      # Convert seconds to HH:MM:SS.ms format\n",
    "      hours = int(timestamp_sec // 3600)\n",
    "      minutes = int((timestamp_sec % 3600) // 60)\n",
    "      seconds = int(timestamp_sec % 60)\n",
    "      milliseconds = int((timestamp_sec % 1) * 1000)\n",
    "      \n",
    "      timestamp_text = f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
    "      \n",
    "      # Create a copy to avoid modifying original\n",
    "      frame_with_timestamp = frame.copy()\n",
    "      \n",
    "      # Get frame dimensions\n",
    "      height, width = frame.shape[:2]\n",
    "      \n",
    "      # Set up text properties\n",
    "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "      font_scale = 0.7\n",
    "      font_thickness = 2\n",
    "      text_color = (255, 255, 255)  # White text\n",
    "      bg_color = (0, 0, 0)  # Black background\n",
    "      padding = 10\n",
    "      \n",
    "      # Get text size\n",
    "      (text_width, text_height), baseline = cv2.getTextSize(\n",
    "          timestamp_text, font, font_scale, font_thickness\n",
    "      )\n",
    "      \n",
    "      # Position in top-right corner\n",
    "      text_x = width - text_width - padding\n",
    "      text_y = padding + text_height\n",
    "      \n",
    "      # Draw semi-transparent background rectangle\n",
    "      bg_x1 = text_x - 5\n",
    "      bg_y1 = text_y - text_height - 5\n",
    "      bg_x2 = text_x + text_width + 5\n",
    "      bg_y2 = text_y + baseline + 5\n",
    "      \n",
    "      # Create overlay for semi-transparency\n",
    "      overlay = frame_with_timestamp.copy()\n",
    "      cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), bg_color, -1)\n",
    "      cv2.addWeighted(overlay, 0.6, frame_with_timestamp, 0.4, 0, frame_with_timestamp)\n",
    "      \n",
    "      # Draw text\n",
    "      cv2.putText(\n",
    "          frame_with_timestamp,\n",
    "          timestamp_text,\n",
    "          (text_x, text_y),\n",
    "          font,\n",
    "          font_scale,\n",
    "          text_color,\n",
    "          font_thickness,\n",
    "          cv2.LINE_AA\n",
    "      )\n",
    "      \n",
    "      return frame_with_timestamp\n",
    "\n",
    "  # Open original video to get frames\n",
    "  print(\"Processing frames and applying masks...\")\n",
    "  cap = cv2.VideoCapture(FILE_URL)\n",
    "  fps = cap.get(cv2.CAP_PROP_FPS) or FPS\n",
    "\n",
    "  # Create a mapping of frame_idx to results\n",
    "  print(f\"Found {len(results)} frames in results\")\n",
    "  result_map = {r[\"frame_idx\"]: r for r in results}\n",
    "\n",
    "  frame_idx = 0\n",
    "  saved_images = []\n",
    "  segmented_images = []\n",
    "\n",
    "  while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "          break\n",
    "      \n",
    "      # Only process frames with segmentation results\n",
    "      if frame_idx in result_map:\n",
    "          # Convert BGR to RGB\n",
    "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "          \n",
    "          res = result_map[frame_idx]\n",
    "          \n",
    "          # if res[\"masks\"]:\n",
    "          #     # Get the first (highest score) mask\n",
    "          #     mask = decode_mask(res[\"masks\"][0])\n",
    "              \n",
    "          #     # Overlay with transparency\n",
    "          #     overlay = rgb_frame.copy()\n",
    "          #     overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "          #     masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "\n",
    "          if res[\"masks\"]:\n",
    "              # Get the first (highest score) mask\n",
    "              masks = [decode_mask(x) for x in res[\"masks\"]]\n",
    "              \n",
    "              # Overlay with transparency\n",
    "              overlay = rgb_frame.copy()\n",
    "              for mask in masks:\n",
    "                  overlay[mask > 0.5] = [0, 255, 0]  # Green overlay\n",
    "              masked_frame = cv2.addWeighted(rgb_frame, 0.7, overlay, 0.3, 0)\n",
    "              \n",
    "          else:\n",
    "              masked_frame = rgb_frame\n",
    "          \n",
    "          # Calculate timestamp for this frame\n",
    "          timestamp_sec = frame_idx / fps\n",
    "          \n",
    "          # Add timestamp overlay (convert back to BGR for cv2 operations, then back to RGB)\n",
    "          masked_frame_bgr = cv2.cvtColor(masked_frame, cv2.COLOR_RGB2BGR)\n",
    "          masked_frame_with_timestamp = add_timestamp(masked_frame_bgr, timestamp_sec)\n",
    "          masked_frame = cv2.cvtColor(masked_frame_with_timestamp, cv2.COLOR_BGR2RGB)\n",
    "          \n",
    "          # Save frame\n",
    "          if not truncate:\n",
    "              saved_images.append(Image.fromarray(rgb_frame))\n",
    "              segmented_images.append(Image.fromarray(masked_frame))\n",
    "          elif res[\"masks\"]:\n",
    "              saved_images.append(Image.fromarray(rgb_frame))\n",
    "              segmented_images.append(Image.fromarray(masked_frame))\n",
    "      \n",
    "      frame_idx += 1\n",
    "\n",
    "  cap.release()\n",
    "  print(f\"Saved {len(saved_images)} frames to memory\")\n",
    "  print(f\"Saved {len(segmented_images)} segmented frames to memory\")\n",
    "\n",
    "  # 3. Create full segmented video\n",
    "  import imageio\n",
    "  import os\n",
    "  import shutil\n",
    "  import tempfile\n",
    "\n",
    "  print(\"Writing video to temporary file...\")\n",
    "  with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:\n",
    "      temp_video_path = tmp_file.name\n",
    "\n",
    "  imageio.mimsave(\n",
    "      temp_video_path,\n",
    "      segmented_images,\n",
    "      fps=(fps/frame_stride) * 5,  # Use original FPS instead of hardcoded 24\n",
    "      codec='libx264',\n",
    "      pixelformat='yuv420p'\n",
    "  )\n",
    "\n",
    "  temp_size = os.path.getsize(temp_video_path)\n",
    "  print(f\"Temporary video created: {temp_size:,} bytes ({temp_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "  # Copy to Volumes\n",
    "  print(f\"Copying to Volumes: {OUTPUT_FILE_URL}\")\n",
    "  shutil.copy2(temp_video_path, OUTPUT_FILE_URL)\n",
    "\n",
    "  final_size = os.path.getsize(OUTPUT_FILE_URL)\n",
    "  print(f\"✓ Video successfully saved to: {OUTPUT_FILE_URL}\")\n",
    "  print(f\"  Final size: {final_size:,} bytes ({final_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "  # Clean up temporary file\n",
    "  if os.path.exists(temp_video_path):\n",
    "      os.remove(temp_video_path)\n",
    "      print(\"Cleaned up temporary file\")\n",
    "\n",
    "  return OUTPUT_FILE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2eb059-b15b-4f7e-99b6-93ed365bbe38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_file(triggered_file, prompt):\n",
    "    print(f\"Triggered by file: {triggered_file}\")\n",
    "    # Your processing logic here\n",
    "    model_input = pd.DataFrame([{\n",
    "        \"video_path\": triggered_file,\n",
    "        \"prompt\": prompt,\n",
    "        \"frame_stride\": frame_stride,  # Process every nth frame\n",
    "        \"batch_size\": 4,\n",
    "        \"threshold\": 0.5,\n",
    "        \"mask_threshold\": 0.5\n",
    "    }])\n",
    "    results = model.predict(model_input)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fe1022-307b-4535-b851-ebb432a377f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed in this run: /Volumes/justinm/cv/auto_segment/inputs/usps_youtube_example.mov\nSegmenting video...\nTriggered by file: /Volumes/justinm/cv/auto_segment/inputs/usps_youtube_example.mov\nInference time: 255 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Files processed in this run:\", most_recent_path)\n",
    "\n",
    "print(\"Segmenting video...\")\n",
    "starting_time = timeit.default_timer()\n",
    "results = process_file(most_recent_path, prompt)\n",
    "print(f\"Inference time: {round((timeit.default_timer() - starting_time))} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22671e7f-aeec-4283-8ea1-4dd5c331d4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frames and applying masks...\nFound 751 frames in results\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (2212, 1238) to (2224, 1248) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 427 frames to memory\nSaved 427 segmented frames to memory\nWriting video to temporary file...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\nINFO:py4j.clientserver:Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 530, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\nINFO:py4j.clientserver:Closing down clientserver connection\nIOStream.flush timed out\nIOStream.flush timed out\nINFO:py4j.clientserver:Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 530, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\nINFO:py4j.clientserver:Closing down clientserver connection\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\nIOStream.flush timed out\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary video created: 18,375,338 bytes (17.52 MB)\nCopying to Volumes: /Volumes/justinm/cv/auto_segment/outputs/usps_youtube_example.mov\n✓ Video successfully saved to: /Volumes/justinm/cv/auto_segment/outputs/usps_youtube_example.mov\n  Final size: 18,375,338 bytes (17.52 MB)\nCleaned up temporary file\n"
     ]
    }
   ],
   "source": [
    "output_file_url = write_results(most_recent_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1604d061-6af4-4358-9dd9-eb27ce96e39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy\n",
    "import base64\n",
    "import cv2\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from openai import AsyncOpenAI, ChatCompletion\n",
    "from typing import NamedTuple, List, Optional, Generator, Tuple\n",
    "\n",
    "# helpers\n",
    "class FrameBatch(NamedTuple):\n",
    "    content: List[dict]\n",
    "    sizes: List[int]\n",
    "    total_bytes: int\n",
    "    \n",
    "    @property\n",
    "    def frame_count(self) -> int:\n",
    "        return len(self.content)\n",
    "\n",
    "def encode_jpeg(frame: np.ndarray, quality: int) -> bytes:\n",
    "    \"\"\"cv2.imencode is 3-5x faster than PIL, no color conversion needed.\"\"\"\n",
    "    _, buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    return buf.tobytes()\n",
    "\n",
    "def to_content(jpg_bytes: bytes) -> dict:\n",
    "    b64 = base64.b64encode(jpg_bytes).decode('ascii')\n",
    "    return {'type': 'image_url', 'image_url': {'url': f'data:image/jpeg;base64,{b64}'}}\n",
    "\n",
    "def fit_to_size(frame: np.ndarray, max_size: int) -> Tuple[bytes, int]:\n",
    "    \n",
    "    # Early exit - most frames fit at q=95\n",
    "    data = encode_jpeg(frame, 95)\n",
    "    if len(data) <= max_size:\n",
    "        return data, 95\n",
    "    \n",
    "    # binary search FTW!\n",
    "    lo, hi, best = 10, 94, (encode_jpeg(frame, 10), 10)\n",
    "    while lo <= hi:\n",
    "        mid = (lo + hi) >> 1\n",
    "        data = encode_jpeg(frame, mid)\n",
    "        if len(data) <= max_size:\n",
    "            best, lo = (data, mid), mid + 1\n",
    "        else:\n",
    "            hi = mid - 1\n",
    "    return best\n",
    "\n",
    "def process_frame(\n",
    "    frame: np.ndarray, \n",
    "    quality: Optional[int] = None,\n",
    "    max_size: int = 500_000\n",
    ") -> Tuple[dict, int, int]:\n",
    "    if quality:\n",
    "        data = encode_jpeg(frame, quality)\n",
    "    else:\n",
    "        data, quality = fit_to_size(frame, max_size)\n",
    "    return to_content(data), quality, len(data)\n",
    "\n",
    "def stream_content(\n",
    "    video: cv2.VideoCapture,\n",
    "    quality: Optional[int] = None,\n",
    "    max_size: int = 500_000\n",
    ") -> Generator[Tuple[dict, int, int], None, None]:\n",
    "    while True:\n",
    "        ok, frame = video.read()\n",
    "        if not ok:\n",
    "            return\n",
    "        yield process_frame(frame, quality, max_size)\n",
    "\n",
    "def batch_content(\n",
    "    video: cv2.VideoCapture,\n",
    "    quality: Optional[int] = None,\n",
    "    max_frame_size: int = 500_000,\n",
    "    max_batch_size: int = 3_000_000\n",
    ") -> Generator[FrameBatch, None, None]:\n",
    "    max_frame_size = min(max_frame_size, max_batch_size)\n",
    "    batch, sizes, batch_size = [], [], 0\n",
    "    for content, _, size in stream_content(video, quality, max_frame_size):\n",
    "        if batch and batch_size + size > max_batch_size:\n",
    "            yield FrameBatch(batch, sizes, batch_size)\n",
    "            batch, sizes, batch_size = [], [], 0\n",
    "        batch.append(content)\n",
    "        sizes.append(size)\n",
    "        batch_size += size\n",
    "    \n",
    "    if batch:\n",
    "        yield FrameBatch(batch, sizes, batch_size)\n",
    "\n",
    "def summarize_frames(frame_batch: FrameBatch):\n",
    "    return oai.chat.completions.create(\n",
    "    model=FMAPI_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': [\n",
    "            {'type': 'text', 'text': \n",
    "                'Describe what is happening in the following sequence of images '\n",
    "                'which represent a frames from a video. Describe what is going on in from the cameras '\n",
    "                'perspective. Be descriptive - but succinct and functional'\n",
    "            },\n",
    "            *frame_batch.content\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "    \n",
    "def get_content(completion: ChatCompletion) -> str:\n",
    "    '''\n",
    "        get_content extracts the textual content.  Required with gemini 3 models\n",
    "        as g3 model's content may (or may not) include a thoughtSignature which \n",
    "        is a reference to the model's reasoning process.\n",
    "    '''\n",
    "    content = completion.choices[0].message.content\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    if isinstance(content, list):\n",
    "        return content[0]['text']\n",
    "    else:\n",
    "        print(type(content))\n",
    "        print(content)\n",
    "        raise Exception('weird content signature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d119a009-93b1-4aa7-b48f-6b3ba8222195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF FMAPI_BASE_URL: https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: POST https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following functional narrative summarizes the warehouse and postal sorting process depicted across the video sections:\n\n### **Phase 1: Demonstration of Manual Sorting**\nThe sequence begins in a warehouse where two presenters, a man and a woman in green uniforms, demonstrate manual package handling. The woman initiates the workflow by moving a cylindrical package across a series of numbered bins (e.g., 24601, 90210) before stacking rectangular boxes on a central work table. The presenters then transition into an instructional mode, using close-up shots to show gloved hands precisely sorting white envelopes and manila folders into specific bins. Throughout this phase, the man provides verbal explanations accompanied by dynamic hand gestures, while the woman assists by pointing out specific bin numbers like \"46516\" and \"90210\" as items are deposited.\n\n### **Phase 2: Collaborative Workflow and Package Handling**\nThe narrative shifts to a more active sorting phase. The two workers operate in tandem around the long white table, which is covered in mail and slips of paper. They systematically pick up items, examine them, and distribute them: the woman focuses on the left-side bins, while the man handles the central bins and a metal shelving unit to the right. The process involves a variety of materials, including small cards, yellow envelopes, and large cardboard boxes. At one point, a coordinated exchange occurs where the man tosses a box to the woman, who catches it while managing two long mailing tubes. The activity concludes with the man organizing the tubes on the shelving unit while the woman clears the remaining items from the table.\n\n### **Phase 3: Historical Context and Industrial Evolution**\nThe video transitions from the modern studio setting to a historical montage detailing the evolution of the U.S. Mail. This functional history begins with manual delivery at stone buildings and horse-drawn carriages, moving into early 20th-century sorting rooms where workers utilized large wooden cubby systems. The narrative then progresses into the mid-century era, showing a woman operating a vintage mechanical sorting machine by feeding envelopes into a vertical hopper at a large console terminal.\n\n### **Phase 4: Modern Automated Systems and Delivery**\nThe final segment focuses on contemporary postal efficiency. It highlights high-speed automated machinery where envelopes move rapidly through complex belts and rollers, monitored by technicians. The workflow extends to the logistics floor, where workers use manual pallet jacks and move large metal mail cages through the warehouse. The process culminates in the final stages of the delivery chain: a worker loads bins into a USPS truck, and a carrier performs a residential delivery, placing mail directly into a curbside mailbox from the vehicle window. The video concludes with the two presenters returning to the screen, standing behind their work table to offer final remarks.\n"
     ]
    }
   ],
   "source": [
    "# actual running code\n",
    "vid_path = output_file_url\n",
    "video = cv2.VideoCapture(vid_path)\n",
    "frame_batches = list(batch_content(video, quality=80))\n",
    "video.release()\n",
    "\n",
    "FMAPI_SERVING_URL = f'https://{spark.conf.get(\"spark.databricks.workspaceUrl\")}'\n",
    "FMAPI_BASE_URL = f'{FMAPI_SERVING_URL}/serving-endpoints'\n",
    "print(f'\uD83C\uDFAF FMAPI_BASE_URL: {FMAPI_BASE_URL}')\n",
    "FMAPI_API_TOKEN = dbutils.secrets.get('auth', 'db-pat')\n",
    "# Serving endpoint I created using my own GCP acc't\n",
    "# to side-step issues w/ limits databricks hosted model issues.\n",
    "FMAPI_MODEL = 'stsu-gemini-3-flash' \n",
    "\n",
    "# clients\n",
    "oai = AsyncOpenAI(\n",
    "    api_key = FMAPI_API_TOKEN,\n",
    "    base_url = FMAPI_BASE_URL\n",
    ")\n",
    "\n",
    "# Get results\n",
    "ai_results = await asyncio.gather(*[\n",
    "    summarize_frames(frame_batch)\n",
    "    for frame_batch in frame_batches\n",
    "])\n",
    "\n",
    "# concatenation of previous generations\n",
    "context = []\n",
    "for idx, result in enumerate(ai_results):\n",
    "    description = f'SECTION {idx}\\n' + get_content(result)\n",
    "    context.append(description)\n",
    "\n",
    "context = '\\n'.join(context)\n",
    "\n",
    "video_summary = await oai.chat.completions.create(\n",
    "    model=FMAPI_MODEL,\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': [\n",
    "            {'type': 'text', 'text': \n",
    "                'Summarize the following sections of a video with a focus on '\n",
    "                'what is functionally happening over time. Be descriptive. '\n",
    "                'The resulting summary should be a functional narrative. '\n",
    "                'Return this as Markdown text - and only markdown. '\n",
    "            },\n",
    "            {'type': 'text', 'text': context}\n",
    "        ]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "text = get_content(video_summary)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f87af10-4ec2-419b-8edf-5ee63de49564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/justinm/cv/auto_segment/descriptions/usps_youtube_example.txt\nWrote 2971 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "print(txt_filename)\n",
    "\n",
    "dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd7d6800-1ea6-4f40-bf19-78a28ef81cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a13c4cd7-4e67-4ac7-ab93-ac59ade2f429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57647b7b-b98d-4d04-8126-2f132f1ffdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Error while sending or receiving.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\nINFO:py4j.clientserver:Closing down clientserver connection\nINFO:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 527, in send_command\n    self.socket.sendall(command.encode(\"utf-8\"))\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 530, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending\nINFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cc937923964fcd8b1bd0b9bee9d5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8c122546714942aa7f1f32497435ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading /local_disk0/user_tmp_data/spark-0c301aa0-0850-4cb6-a84e-bb/tmp4b9vssnh/python_model.pkl:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-image-captioning-large/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/chat_template.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/audio_tokenizer_config.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/preprocessor_config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/preprocessor_config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/processor_config.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/preprocessor_config.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/preprocessor_config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/tokenizer_config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/tokenizer_config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-image-captioning-large/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-image-captioning-large/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/vocab.txt \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/vocab.txt \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/vocab.txt \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/tokenizer.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/tokenizer.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/special_tokens_map.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/special_tokens_map.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/config.json \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\nINFO:httpx:HTTP Request: GET https://huggingface.co/api/models/Salesforce/blip-image-captioning-large/xet-read-token/353689b859fcf0523410b1806dace5fb46ecdf41 \"HTTP/1.1 200 OK\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/generation_config.json \"HTTP/1.1 404 Not Found\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/Salesforce/blip-image-captioning-large/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\nINFO:httpx:HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/Salesforce/blip-image-captioning-large/353689b859fcf0523410b1806dace5fb46ecdf41/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# captioning_model = mlflow.pyfunc.load_model(\"models:/justinm.cv.transformers-blip@job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed8dcfa-be29-446c-b634-ce4fb9272764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "# import base64\n",
    "\n",
    "# def pil_to_base64_str(img: Image.Image, format: str = \"PNG\") -> str:\n",
    "#     \"\"\"\n",
    "#     Convert a PIL image to a Base64-encoded string.\n",
    "    \n",
    "#     Args:\n",
    "#         img: PIL.Image.Image\n",
    "#         format: image format, e.g., 'PNG' or 'JPEG'\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Base64 string that can be safely passed in JSON\n",
    "#     \"\"\"\n",
    "#     buf = BytesIO()\n",
    "#     img.save(buf, format=format)\n",
    "#     buf.seek(0)\n",
    "#     b64_str = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "#     return b64_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239588f-5b93-4e7e-afe9-9912078ee025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# from io import BytesIO\n",
    "\n",
    "# written_images_str = [pil_to_base64_str(x) for x in written_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a37d75f-a512-4509-a011-65f71d0e0663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model_input = {\n",
    "#     \"image_path\": written_images_str\n",
    "# }\n",
    "\n",
    "# captions = captioning_model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3e8f28-0261-4bc9-8130-e5875c1c7bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Scene 1 – Opening:** A man and a woman stand together in front of a table, facing the camera.  \n\n**Scene 2 – Table details:** The same pair is shown with various items on the table – a cup, boxes, and a painting of a man – while they continue to stand side‑by‑side.  \n\n**Scene 3 – Green‑glove recycling:** A person wearing green gloves handles green‑colored gloves and places pieces of paper into three nearby bins.  \n\n**Scene 4 – Return to the table:** The man and woman re‑appear, again standing next to each other in front of the same table, looking engaged in a discussion.  \n\n**Scene 5 – Cutting paper:** Close‑ups show a hand (first a man, then a woman) cutting a sheet of paper on the table with scissors.  \n\n**Scene 6 – Warehouse & boxes:** The setting shifts to a warehouse‑style room where the man and woman move, lift, and arrange several cardboard boxes on and around a table.  \n\n**Scene 7 – Archival photos:** A short montage of black‑and‑white photographs shows a man in a green suit walking or running up and down a staircase, sometimes beside a horse‑drawn carriage.  \n\n**Scene 8 – Loading the truck:** A man dressed in a green suit or coverall works inside the warehouse, loading boxes onto a white/postal truck and standing beside the truck’s back door.  \n\n**Scene 9 – Mail‑truck & mailbox:** The footage shows a truck with a mailbox and a green‑plumed bird perched on the mailbox, followed by a driver (or rider) sitting in the truck’s seat.  \n\n**Scene 10 – Cardboard‑box branding:** The final segment focuses on cardboard boxes that feature a large green‑clad “green man” graphic; a woman stands next to or in front of these boxes while a man appears in the background.  \n"
     ]
    }
   ],
   "source": [
    "# from databricks.sdk import WorkspaceClient\n",
    "# from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "\n",
    "# # Initialize the client\n",
    "# w = WorkspaceClient()\n",
    "\n",
    "# # Define your endpoint name\n",
    "# endpoint_name = \"databricks-gpt-oss-120b\"\n",
    "\n",
    "# # # Create a chat message\n",
    "# # messages = [\n",
    "# #     ChatMessage(\n",
    "# #         role=ChatMessageRole.USER,\n",
    "# #         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. Can you identify anything out of the ordinary or anomalous in the video? When I say anomlous, I mean actions taking place that are against the law or violent in nature. If you find something that shouldn't be happening, describe why it is anomolous. If you don't find anything abnormal, just respond with 'Nothing anomolous found.' \\n\\n {captions}\"\n",
    "# #     )\n",
    "# # ]\n",
    "# # Create a chat message\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=ChatMessageRole.USER,\n",
    "#         content=f\"The following is a list of descriptions of images. The images represent frames from a video, in order. There may be obvious scene changes based on the descriptions. Please summarize the contents of each scene in the video in a single sentence each.' \\n\\n {captions}\"\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# # Query the endpoint\n",
    "# response = w.serving_endpoints.query(\n",
    "#     name=endpoint_name,\n",
    "#     messages=messages,\n",
    "#     # max_tokens=500  # optional parameter\n",
    "# )\n",
    "\n",
    "# # Access the response\n",
    "# r = response.choices[0].message.content\n",
    "# # print(r)\n",
    "# text = [x for x in r if x['type'] == 'text'][0]['text']\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cf504d-be11-4eca-a71e-55b12c120b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/justinm/cv/auto_segment/descriptions/usps_youtube_example.txt\nWrote 1773 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt_filename = most_recent_path.split('.')[0] + '.txt'\n",
    "# txt_filename = txt_filename.replace('/inputs/', '/descriptions/')\n",
    "# print(txt_filename)\n",
    "\n",
    "# dbutils.fs.put(txt_filename, text, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc51046-5099-4b11-97cc-1be7bb9771f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "imageio",
     "imageio[ffmpeg]",
     "imageio[pyav]",
     "git+https://github.com/huggingface/transformers.git",
     "opencv-python",
     "accelerate>=0.20.0",
     "nvidia-ml-py",
     "openai"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "auto-segment-video-job-fmapi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}